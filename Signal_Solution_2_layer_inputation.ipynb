{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # Tensorflow of ANN\n",
    "import pandas as pd # Pandas DataFrame\n",
    "import numpy as np # Numpy array manipulation\n",
    "from sklearn.model_selection import KFold # k-fold cross validation\n",
    "from sklearn.metrics import accuracy_score, f1_score # Evaluation metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer # Missing value imputation\n",
    "from keras.utils import to_categorical # Multi-class labels converted to one-hot encoded categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence # Keras input preprocessing\n",
    "from keras.models import Sequential # Keras model types\n",
    "from keras.layers import Dense, Dropout, Activation # Keras model layers\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_fcn_block():\n",
    "    ip = Input(shape = (20, 300))\n",
    "    \n",
    "    # Recurrent side\n",
    "    x = LSTM(8)(ip)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Fully Convolutional Side\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding = \"same\", kernel_initializer = \"he_uniform\")(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding = \"same\", kernel_initializer = \"he_uniform\")(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding = \"same\", kernel_initializer = \"he_uniform\")(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    # Merge both sides back together\n",
    "    x = concatenate([x, y])\n",
    "    \n",
    "    # 2 output classes over softmax\n",
    "    out = Dense(2, activation = \"softmax\")(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "training_data = pd.read_csv('train_signal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>...</th>\n",
       "      <th>X5990</th>\n",
       "      <th>X5991</th>\n",
       "      <th>X5992</th>\n",
       "      <th>X5993</th>\n",
       "      <th>X5994</th>\n",
       "      <th>X5995</th>\n",
       "      <th>X5996</th>\n",
       "      <th>X5997</th>\n",
       "      <th>X5998</th>\n",
       "      <th>X5999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00000</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>-1.072</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>N</td>\n",
       "      <td>2.762</td>\n",
       "      <td>3.313</td>\n",
       "      <td>3.863</td>\n",
       "      <td>4.292</td>\n",
       "      <td>4.594</td>\n",
       "      <td>4.623</td>\n",
       "      <td>4.408</td>\n",
       "      <td>3.817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00002</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00003</td>\n",
       "      <td>~</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.672</td>\n",
       "      <td>1.895</td>\n",
       "      <td>2.012</td>\n",
       "      <td>2.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00004</td>\n",
       "      <td>~</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID Type     X0     X1     X2     X3     X4     X5     X6     X7  ...  \\\n",
       "0  B00000    N -0.107 -0.100 -0.086 -0.078 -0.071 -0.057 -0.049 -0.035  ...   \n",
       "1  B00001    N  2.762  3.313  3.863  4.292  4.594  4.623  4.408  3.817  ...   \n",
       "2  B00002    N -0.246 -0.200 -0.159 -0.125 -0.101 -0.090 -0.084 -0.078  ...   \n",
       "3  B00003    ~  0.519  0.778  1.073  1.392  1.672  1.895  2.012  2.023  ...   \n",
       "4  B00004    ~  0.011 -0.103 -0.265 -0.371 -0.409 -0.422 -0.418 -0.411  ...   \n",
       "\n",
       "   X5990  X5991  X5992  X5993  X5994  X5995  X5996  X5997  X5998  X5999  \n",
       "0 -1.108 -1.072 -1.028 -0.978 -0.912 -0.862 -0.804 -0.724 -0.630 -0.499  \n",
       "1 -0.107  0.003  0.148  0.241  0.310  0.345  0.368  0.397  0.426  0.438  \n",
       "2 -0.113 -0.038  0.032  0.107  0.165  0.194  0.194  0.159  0.119  0.072  \n",
       "3  0.037 -0.052 -0.084 -0.099 -0.101 -0.090 -0.067  0.003  0.096  0.179  \n",
       "4  0.776  0.829  0.763  0.481  0.126 -0.144 -0.224 -0.250 -0.222 -0.207  \n",
       "\n",
       "[5 rows x 6002 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at the data\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duplicate of the training_data for use in 1st layer\n",
    "training_data_l1 = training_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to 2 classes for 2 layer classification\n",
    "\n",
    "# Other & Noisy together\n",
    "training_data_l1.loc[training_data_l1['Type'] == 'O', 'Type'] = 0\n",
    "training_data_l1.loc[training_data_l1['Type'] == 'N', 'Type'] = 0\n",
    "\n",
    "# AF and noisy together\n",
    "training_data_l1.loc[training_data_l1['Type'] == 'A', 'Type'] = 1\n",
    "training_data_l1.loc[training_data_l1['Type'] == '~', 'Type'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>...</th>\n",
       "      <th>X5990</th>\n",
       "      <th>X5991</th>\n",
       "      <th>X5992</th>\n",
       "      <th>X5993</th>\n",
       "      <th>X5994</th>\n",
       "      <th>X5995</th>\n",
       "      <th>X5996</th>\n",
       "      <th>X5997</th>\n",
       "      <th>X5998</th>\n",
       "      <th>X5999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>-1.072</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>0</td>\n",
       "      <td>2.762</td>\n",
       "      <td>3.313</td>\n",
       "      <td>3.863</td>\n",
       "      <td>4.292</td>\n",
       "      <td>4.594</td>\n",
       "      <td>4.623</td>\n",
       "      <td>4.408</td>\n",
       "      <td>3.817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00002</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.672</td>\n",
       "      <td>1.895</td>\n",
       "      <td>2.012</td>\n",
       "      <td>2.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Type     X0     X1     X2     X3     X4     X5     X6     X7  ...  \\\n",
       "0  B00000     0 -0.107 -0.100 -0.086 -0.078 -0.071 -0.057 -0.049 -0.035  ...   \n",
       "1  B00001     0  2.762  3.313  3.863  4.292  4.594  4.623  4.408  3.817  ...   \n",
       "2  B00002     0 -0.246 -0.200 -0.159 -0.125 -0.101 -0.090 -0.084 -0.078  ...   \n",
       "3  B00003     1  0.519  0.778  1.073  1.392  1.672  1.895  2.012  2.023  ...   \n",
       "4  B00004     1  0.011 -0.103 -0.265 -0.371 -0.409 -0.422 -0.418 -0.411  ...   \n",
       "\n",
       "   X5990  X5991  X5992  X5993  X5994  X5995  X5996  X5997  X5998  X5999  \n",
       "0 -1.108 -1.072 -1.028 -0.978 -0.912 -0.862 -0.804 -0.724 -0.630 -0.499  \n",
       "1 -0.107  0.003  0.148  0.241  0.310  0.345  0.368  0.397  0.426  0.438  \n",
       "2 -0.113 -0.038  0.032  0.107  0.165  0.194  0.194  0.159  0.119  0.072  \n",
       "3  0.037 -0.052 -0.084 -0.099 -0.101 -0.090 -0.067  0.003  0.096  0.179  \n",
       "4  0.776  0.829  0.763  0.481  0.126 -0.144 -0.224 -0.250 -0.222 -0.207  \n",
       "\n",
       "[5 rows x 6002 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at the data\n",
    "training_data_l1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into labels (y) and input (X)\n",
    "\n",
    "# Data from the 3rd feature column onwards are input\n",
    "X = training_data_l1.values[:,2:]\n",
    "# Classes/Labels are the type of AF\n",
    "y = training_data_l1[\"Type\"].values\n",
    "# Label is a binary integer\n",
    "y=y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing data\n",
    "\n",
    "# Replace missing values with Nan\n",
    "X[X == ''] = np.nan\n",
    "\n",
    "# Replace Nan with median\n",
    "imputer = Imputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (13062, 6000)\n",
      "y shape: (13062,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13062,)\n",
      "(13062, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode class labels\n",
    "print(y.shape)\n",
    "y = to_categorical(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input as 20 seconds of recording (for recurrent network)\n",
    "X_3d = X.reshape((13062, 20, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (13062, 20, 300)\n",
      "y shape: (13062, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape: \" + str(X_3d.shape))\n",
    "print(\"y shape: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 300, 20)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 300, 128)     20608       permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 300, 128)     512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300, 128)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 300, 256)     164096      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 300, 256)     1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 300, 256)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 300, 128)     98432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 300, 128)     512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 8)            9888        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 295,346\n",
      "Trainable params: 294,322\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build layer 1 classifier\n",
    "model_l1 = lstm_fcn_block()\n",
    "# Tell model what loss function & optimiser to use\n",
    "model_l1.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10885 samples, validate on 2177 samples\n",
      "Epoch 1/15\n",
      "10885/10885 [==============================] - 65s 6ms/step - loss: 0.3617 - accuracy: 0.8876 - auc: 0.8846 - precision: 0.8792 - recall: 0.8792 - val_loss: 0.3731 - val_accuracy: 0.8755 - val_auc: 0.8942 - val_precision: 0.8856 - val_recall: 0.8856\n",
      "Epoch 2/15\n",
      "10885/10885 [==============================] - 65s 6ms/step - loss: 0.3449 - accuracy: 0.8881 - auc: 0.8980 - precision: 0.8858 - recall: 0.8858 - val_loss: 0.3727 - val_accuracy: 0.8755 - val_auc: 0.8996 - val_precision: 0.8858 - val_recall: 0.8858\n",
      "Epoch 3/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3353 - accuracy: 0.8889 - auc: 0.9023 - precision: 0.8854 - recall: 0.8854 - val_loss: 0.3789 - val_accuracy: 0.8755 - val_auc: 0.9046 - val_precision: 0.8861 - val_recall: 0.8861\n",
      "Epoch 4/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3316 - accuracy: 0.8878 - auc: 0.9069 - precision: 0.8865 - recall: 0.8865 - val_loss: 0.3739 - val_accuracy: 0.8741 - val_auc: 0.9078 - val_precision: 0.8860 - val_recall: 0.8860\n",
      "Epoch 5/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3257 - accuracy: 0.8878 - auc: 0.9098 - precision: 0.8863 - recall: 0.8863 - val_loss: 0.3693 - val_accuracy: 0.8751 - val_auc: 0.9107 - val_precision: 0.8859 - val_recall: 0.8859\n",
      "Epoch 6/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3241 - accuracy: 0.8888 - auc: 0.9121 - precision: 0.8862 - recall: 0.8862 - val_loss: 0.3701 - val_accuracy: 0.8755 - val_auc: 0.9128 - val_precision: 0.8860 - val_recall: 0.8860\n",
      "Epoch 7/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3196 - accuracy: 0.8890 - auc: 0.9140 - precision: 0.8862 - recall: 0.8862 - val_loss: 0.3766 - val_accuracy: 0.8732 - val_auc: 0.9148 - val_precision: 0.8861 - val_recall: 0.8861\n",
      "Epoch 8/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3146 - accuracy: 0.8897 - auc: 0.9158 - precision: 0.8863 - recall: 0.8863 - val_loss: 0.3938 - val_accuracy: 0.8764 - val_auc: 0.9163 - val_precision: 0.8862 - val_recall: 0.8862\n",
      "Epoch 9/15\n",
      "10885/10885 [==============================] - 65s 6ms/step - loss: 0.3156 - accuracy: 0.8892 - auc: 0.9170 - precision: 0.8863 - recall: 0.8863 - val_loss: 0.3750 - val_accuracy: 0.8741 - val_auc: 0.9176 - val_precision: 0.8863 - val_recall: 0.8863\n",
      "Epoch 10/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3085 - accuracy: 0.8890 - auc: 0.9186 - precision: 0.8865 - recall: 0.8865 - val_loss: 0.3786 - val_accuracy: 0.8764 - val_auc: 0.9190 - val_precision: 0.8864 - val_recall: 0.8864\n",
      "Epoch 11/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3041 - accuracy: 0.8905 - auc: 0.9199 - precision: 0.8866 - recall: 0.8866 - val_loss: 0.4044 - val_accuracy: 0.8728 - val_auc: 0.9203 - val_precision: 0.8865 - val_recall: 0.8865\n",
      "Epoch 12/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.3007 - accuracy: 0.8913 - auc: 0.9210 - precision: 0.8866 - recall: 0.8866 - val_loss: 0.4112 - val_accuracy: 0.8760 - val_auc: 0.9214 - val_precision: 0.8867 - val_recall: 0.8867\n",
      "Epoch 13/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.2928 - accuracy: 0.8917 - auc: 0.9222 - precision: 0.8869 - recall: 0.8869 - val_loss: 0.3943 - val_accuracy: 0.8659 - val_auc: 0.9228 - val_precision: 0.8868 - val_recall: 0.8868\n",
      "Epoch 14/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.2933 - accuracy: 0.8908 - auc: 0.9236 - precision: 0.8870 - recall: 0.8870 - val_loss: 0.3937 - val_accuracy: 0.8686 - val_auc: 0.9240 - val_precision: 0.8868 - val_recall: 0.8868\n",
      "Epoch 15/15\n",
      "10885/10885 [==============================] - 64s 6ms/step - loss: 0.2863 - accuracy: 0.8944 - auc: 0.9247 - precision: 0.8869 - recall: 0.8869 - val_loss: 0.4081 - val_accuracy: 0.8737 - val_auc: 0.9251 - val_precision: 0.8870 - val_recall: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3f709faf98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model_l1.fit(X_3d, y, epochs=15, batch_size=32, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96460897 0.03539106]\n",
      " [0.96638197 0.03361809]\n",
      " [0.9613774  0.03862257]\n",
      " ...\n",
      " [0.9467135  0.05328647]\n",
      " [0.9835247  0.01647536]\n",
      " [0.9129899  0.08701008]]\n"
     ]
    }
   ],
   "source": [
    "predictions_from_first_layer = model_l1.predict(X_3d)\n",
    "print(predictions_from_first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13062, 6002)\n",
      "13062\n"
     ]
    }
   ],
   "source": [
    "# Check shape is correct\n",
    "print(training_data.shape)\n",
    "print(len(predictions_from_first_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11578, 6002)\n",
      "(1484, 6002)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>...</th>\n",
       "      <th>X5990</th>\n",
       "      <th>X5991</th>\n",
       "      <th>X5992</th>\n",
       "      <th>X5993</th>\n",
       "      <th>X5994</th>\n",
       "      <th>X5995</th>\n",
       "      <th>X5996</th>\n",
       "      <th>X5997</th>\n",
       "      <th>X5998</th>\n",
       "      <th>X5999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00000</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>-1.072</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>N</td>\n",
       "      <td>2.762</td>\n",
       "      <td>3.313</td>\n",
       "      <td>3.863</td>\n",
       "      <td>4.292</td>\n",
       "      <td>4.594</td>\n",
       "      <td>4.623</td>\n",
       "      <td>4.408</td>\n",
       "      <td>3.817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00002</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00003</td>\n",
       "      <td>~</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.672</td>\n",
       "      <td>1.895</td>\n",
       "      <td>2.012</td>\n",
       "      <td>2.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00004</td>\n",
       "      <td>~</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID Type     X0     X1     X2     X3     X4     X5     X6     X7  ...  \\\n",
       "0  B00000    N -0.107 -0.100 -0.086 -0.078 -0.071 -0.057 -0.049 -0.035  ...   \n",
       "1  B00001    N  2.762  3.313  3.863  4.292  4.594  4.623  4.408  3.817  ...   \n",
       "2  B00002    N -0.246 -0.200 -0.159 -0.125 -0.101 -0.090 -0.084 -0.078  ...   \n",
       "3  B00003    ~  0.519  0.778  1.073  1.392  1.672  1.895  2.012  2.023  ...   \n",
       "4  B00004    ~  0.011 -0.103 -0.265 -0.371 -0.409 -0.422 -0.418 -0.411  ...   \n",
       "\n",
       "   X5990  X5991  X5992  X5993  X5994  X5995  X5996  X5997  X5998  X5999  \n",
       "0 -1.108 -1.072 -1.028 -0.978 -0.912 -0.862 -0.804 -0.724 -0.630 -0.499  \n",
       "1 -0.107  0.003  0.148  0.241  0.310  0.345  0.368  0.397  0.426  0.438  \n",
       "2 -0.113 -0.038  0.032  0.107  0.165  0.194  0.194  0.159  0.119  0.072  \n",
       "3  0.037 -0.052 -0.084 -0.099 -0.101 -0.090 -0.067  0.003  0.096  0.179  \n",
       "4  0.776  0.829  0.763  0.481  0.126 -0.144 -0.224 -0.250 -0.222 -0.207  \n",
       "\n",
       "[5 rows x 6002 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into two datasets for 2nd layer classification\n",
    "training_data_l2_NaO = training_data.loc[(training_data['Type'] == 'N') | (training_data['Type'] == 'O')]\n",
    "training_data_l2_AaN = training_data.loc[(training_data['Type'] == 'A') | (training_data['Type'] == '~')]\n",
    "\n",
    "print(training_data_l2_NaO.shape)\n",
    "print(training_data_l2_AaN.shape)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>...</th>\n",
       "      <th>X5990</th>\n",
       "      <th>X5991</th>\n",
       "      <th>X5992</th>\n",
       "      <th>X5993</th>\n",
       "      <th>X5994</th>\n",
       "      <th>X5995</th>\n",
       "      <th>X5996</th>\n",
       "      <th>X5997</th>\n",
       "      <th>X5998</th>\n",
       "      <th>X5999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>-1.072</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>1</td>\n",
       "      <td>2.762</td>\n",
       "      <td>3.313</td>\n",
       "      <td>3.863</td>\n",
       "      <td>4.292</td>\n",
       "      <td>4.594</td>\n",
       "      <td>4.623</td>\n",
       "      <td>4.408</td>\n",
       "      <td>3.817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00002</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00007</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.634</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00008</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Type     X0     X1     X2     X3     X4     X5     X6     X7  ...  \\\n",
       "0  B00000     1 -0.107 -0.100 -0.086 -0.078 -0.071 -0.057 -0.049 -0.035  ...   \n",
       "1  B00001     1  2.762  3.313  3.863  4.292  4.594  4.623  4.408  3.817  ...   \n",
       "2  B00002     1 -0.246 -0.200 -0.159 -0.125 -0.101 -0.090 -0.084 -0.078  ...   \n",
       "7  B00007     0 -0.280 -0.481 -0.546 -0.594 -0.651 -0.659 -0.634 -0.626  ...   \n",
       "8  B00008     0 -0.393 -0.336 -0.288 -0.256 -0.224 -0.191 -0.159 -0.135  ...   \n",
       "\n",
       "   X5990  X5991  X5992  X5993  X5994  X5995  X5996  X5997  X5998  X5999  \n",
       "0 -1.108 -1.072 -1.028 -0.978 -0.912 -0.862 -0.804 -0.724 -0.630 -0.499  \n",
       "1 -0.107  0.003  0.148  0.241  0.310  0.345  0.368  0.397  0.426  0.438  \n",
       "2 -0.113 -0.038  0.032  0.107  0.165  0.194  0.194  0.159  0.119  0.072  \n",
       "7 -0.046 -0.022 -0.006  0.010  0.018  0.018  0.026  0.026  0.034  0.034  \n",
       "8 -0.280 -0.256 -0.232 -0.207 -0.183 -0.151 -0.127 -0.103 -0.087 -0.078  \n",
       "\n",
       "[5 rows x 6002 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary class encoding\n",
    "training_data_l2_NaO.loc[training_data_l2_NaO['Type'] == 'O', 'Type'] = 0\n",
    "training_data_l2_NaO.loc[training_data_l2_NaO['Type'] == 'N', 'Type'] = 1\n",
    "training_data_l2_NaO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into labels (y) and input (X)\n",
    "\n",
    "# Data from the 3rd feature column onwards are input\n",
    "X = training_data_l2_NaO.values[:,2:]\n",
    "# Classes/Labels are the type of AF\n",
    "y = training_data_l2_NaO[\"Type\"].values\n",
    "# Label is a binary integer\n",
    "y=y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing data\n",
    "\n",
    "# Replace missing values with Nan\n",
    "X[X == ''] = np.nan\n",
    "\n",
    "# Replace Nan with median\n",
    "imputer = Imputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11578,)\n",
      "(11578, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode class labels\n",
    "print(y.shape)\n",
    "y = to_categorical(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input as 20 seconds of recording (for recurrent network)\n",
    "X_3d = X.reshape((11578, 20, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (11578, 20, 300)\n",
      "y shape: (11578, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape: \" + str(X_3d.shape))\n",
    "print(\"y shape: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 300, 20)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 300, 128)     20608       permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 300, 128)     512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 300, 128)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 300, 256)     164096      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 300, 256)     1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 300, 256)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 300, 128)     98432       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 300, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 8)            9888        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 300, 128)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8)            0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 136)          0           dropout_2[0][0]                  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            274         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 295,346\n",
      "Trainable params: 294,322\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build layer 2 Normal & Other classifier\n",
    "model_l2_NaO = lstm_fcn_block()\n",
    "# Tell model what loss function & optimiser to use\n",
    "model_l2_NaO.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9648 samples, validate on 1930 samples\n",
      "Epoch 1/15\n",
      "9648/9648 [==============================] - 58s 6ms/step - loss: 0.6479 - accuracy: 0.6545 - auc_1: 0.6684 - precision_1: 0.6392 - recall_1: 0.6392 - val_loss: 0.6080 - val_accuracy: 0.6850 - val_auc_1: 0.6879 - val_precision_1: 0.6581 - val_recall_1: 0.6581\n",
      "Epoch 2/15\n",
      "9648/9648 [==============================] - 58s 6ms/step - loss: 0.6233 - accuracy: 0.6652 - auc_1: 0.6976 - precision_1: 0.6613 - recall_1: 0.6613 - val_loss: 0.6272 - val_accuracy: 0.6544 - val_auc_1: 0.7012 - val_precision_1: 0.6621 - val_recall_1: 0.6621\n",
      "Epoch 3/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.6140 - accuracy: 0.6731 - auc_1: 0.7059 - precision_1: 0.6640 - recall_1: 0.6640 - val_loss: 0.6104 - val_accuracy: 0.6870 - val_auc_1: 0.7092 - val_precision_1: 0.6656 - val_recall_1: 0.6656\n",
      "Epoch 4/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.6079 - accuracy: 0.6768 - auc_1: 0.7135 - precision_1: 0.6683 - recall_1: 0.6683 - val_loss: 0.6189 - val_accuracy: 0.6705 - val_auc_1: 0.7148 - val_precision_1: 0.6686 - val_recall_1: 0.6686\n",
      "Epoch 5/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.6045 - accuracy: 0.6827 - auc_1: 0.7170 - precision_1: 0.6700 - recall_1: 0.6700 - val_loss: 0.6231 - val_accuracy: 0.6637 - val_auc_1: 0.7184 - val_precision_1: 0.6708 - val_recall_1: 0.6708\n",
      "Epoch 6/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.6002 - accuracy: 0.6822 - auc_1: 0.7205 - precision_1: 0.6721 - recall_1: 0.6721 - val_loss: 0.6055 - val_accuracy: 0.6813 - val_auc_1: 0.7221 - val_precision_1: 0.6726 - val_recall_1: 0.6726\n",
      "Epoch 7/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.5932 - accuracy: 0.6892 - auc_1: 0.7241 - precision_1: 0.6738 - recall_1: 0.6738 - val_loss: 0.6244 - val_accuracy: 0.6554 - val_auc_1: 0.7255 - val_precision_1: 0.6744 - val_recall_1: 0.6744\n",
      "Epoch 8/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.5955 - accuracy: 0.6821 - auc_1: 0.7268 - precision_1: 0.6748 - recall_1: 0.6748 - val_loss: 0.6203 - val_accuracy: 0.6642 - val_auc_1: 0.7275 - val_precision_1: 0.6749 - val_recall_1: 0.6749\n",
      "Epoch 9/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.5928 - accuracy: 0.6906 - auc_1: 0.7286 - precision_1: 0.6755 - recall_1: 0.6755 - val_loss: 0.6092 - val_accuracy: 0.6829 - val_auc_1: 0.7297 - val_precision_1: 0.6765 - val_recall_1: 0.6765\n",
      "Epoch 10/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.5789 - accuracy: 0.6963 - auc_1: 0.7313 - precision_1: 0.6774 - recall_1: 0.6774 - val_loss: 0.6187 - val_accuracy: 0.6803 - val_auc_1: 0.7328 - val_precision_1: 0.6781 - val_recall_1: 0.6781\n",
      "Epoch 11/15\n",
      "9648/9648 [==============================] - 58s 6ms/step - loss: 0.5757 - accuracy: 0.7014 - auc_1: 0.7342 - precision_1: 0.6790 - recall_1: 0.6790 - val_loss: 0.6160 - val_accuracy: 0.6808 - val_auc_1: 0.7354 - val_precision_1: 0.6799 - val_recall_1: 0.6799\n",
      "Epoch 12/15\n",
      "9648/9648 [==============================] - 58s 6ms/step - loss: 0.5670 - accuracy: 0.7112 - auc_1: 0.7372 - precision_1: 0.6813 - recall_1: 0.6813 - val_loss: 0.6256 - val_accuracy: 0.6627 - val_auc_1: 0.7383 - val_precision_1: 0.6820 - val_recall_1: 0.6820\n",
      "Epoch 13/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.5605 - accuracy: 0.7152 - auc_1: 0.7399 - precision_1: 0.6831 - recall_1: 0.6831 - val_loss: 0.6304 - val_accuracy: 0.6565 - val_auc_1: 0.7411 - val_precision_1: 0.6838 - val_recall_1: 0.6838\n",
      "Epoch 14/15\n",
      "9648/9648 [==============================] - 57s 6ms/step - loss: 0.5536 - accuracy: 0.7210 - auc_1: 0.7429 - precision_1: 0.6851 - recall_1: 0.6851 - val_loss: 0.6197 - val_accuracy: 0.6762 - val_auc_1: 0.7439 - val_precision_1: 0.6859 - val_recall_1: 0.6859\n",
      "Epoch 15/15\n",
      "9648/9648 [==============================] - 58s 6ms/step - loss: 0.5486 - accuracy: 0.7250 - auc_1: 0.7457 - precision_1: 0.6870 - recall_1: 0.6870 - val_loss: 0.7131 - val_accuracy: 0.5881 - val_auc_1: 0.7464 - val_precision_1: 0.6875 - val_recall_1: 0.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3f734f9e10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model_l2_NaO.fit(X_3d, y, epochs=15, batch_size=32, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>...</th>\n",
       "      <th>X5990</th>\n",
       "      <th>X5991</th>\n",
       "      <th>X5992</th>\n",
       "      <th>X5993</th>\n",
       "      <th>X5994</th>\n",
       "      <th>X5995</th>\n",
       "      <th>X5996</th>\n",
       "      <th>X5997</th>\n",
       "      <th>X5998</th>\n",
       "      <th>X5999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.672</td>\n",
       "      <td>1.895</td>\n",
       "      <td>2.012</td>\n",
       "      <td>2.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00004</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.157</td>\n",
       "      <td>1.349</td>\n",
       "      <td>1.542</td>\n",
       "      <td>1.755</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.708</td>\n",
       "      <td>-1.650</td>\n",
       "      <td>-1.563</td>\n",
       "      <td>-1.457</td>\n",
       "      <td>-1.351</td>\n",
       "      <td>-1.255</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>-1.033</td>\n",
       "      <td>-0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00006</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.409</td>\n",
       "      <td>-1.274</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B00009</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.638</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>...</td>\n",
       "      <td>1.419</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.044</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Type     X0     X1     X2     X3     X4     X5     X6     X7  ...  \\\n",
       "3  B00003     0  0.519  0.778  1.073  1.392  1.672  1.895  2.012  2.023  ...   \n",
       "4  B00004     0  0.011 -0.103 -0.265 -0.371 -0.409 -0.422 -0.418 -0.411  ...   \n",
       "5  B00005     0  0.539  0.655  0.800  0.964  1.157  1.349  1.542  1.755  ...   \n",
       "6  B00006     0 -0.927 -0.869 -0.782 -0.657 -0.473 -0.348 -0.261 -0.203  ...   \n",
       "9  B00009     0 -0.130 -0.211 -0.308 -0.421 -0.531 -0.638 -0.748 -0.861  ...   \n",
       "\n",
       "   X5990  X5991  X5992  X5993  X5994  X5995  X5996  X5997  X5998  X5999  \n",
       "3  0.037 -0.052 -0.084 -0.099 -0.101 -0.090 -0.067  0.003  0.096  0.179  \n",
       "4  0.776  0.829  0.763  0.481  0.126 -0.144 -0.224 -0.250 -0.222 -0.207  \n",
       "5 -1.708 -1.650 -1.563 -1.457 -1.351 -1.255 -1.158 -1.100 -1.033 -0.985  \n",
       "6 -1.409 -1.274 -1.120 -0.927 -0.657 -0.493 -0.396 -0.338 -0.309 -0.338  \n",
       "9  1.419  1.199  1.044  0.992  0.960  0.944  0.908  0.850  0.775  0.701  \n",
       "\n",
       "[5 rows x 6002 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary class encoding\n",
    "training_data_l2_AaN.loc[training_data_l2_AaN['Type'] == '~', 'Type'] = 0\n",
    "training_data_l2_AaN.loc[training_data_l2_AaN['Type'] == 'A', 'Type'] = 1\n",
    "training_data_l2_AaN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into labels (y) and input (X)\n",
    "\n",
    "# Data from the 3rd feature column onwards are input\n",
    "X = training_data_l2_AaN.values[:,2:]\n",
    "# Classes/Labels are the type of AF\n",
    "y = training_data_l2_AaN[\"Type\"].values\n",
    "# Label is a binary integer\n",
    "y=y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing data\n",
    "\n",
    "# Replace missing values with Nan\n",
    "X[X == ''] = np.nan\n",
    "\n",
    "# Replace Nan with median\n",
    "imputer = Imputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1484,)\n",
      "(1484, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode class labels\n",
    "print(y.shape)\n",
    "y = to_categorical(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input as 20 seconds of recording (for recurrent network)\n",
    "X_3d = X.reshape((1484, 20, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1484, 20, 300)\n",
      "y shape: (1484, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape: \" + str(X_3d.shape))\n",
    "print(\"y shape: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 20, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 300, 20)      0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 300, 128)     20608       permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 300, 128)     512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 300, 128)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 300, 256)     164096      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 300, 256)     1024        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 300, 256)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 300, 128)     98432       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 300, 128)     512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 8)            9888        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 300, 128)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8)            0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 136)          0           dropout_3[0][0]                  \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            274         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 295,346\n",
      "Trainable params: 294,322\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build layer 2 AF & Noisy classifier\n",
    "model_l2_AaN = lstm_fcn_block()\n",
    "# Tell model what loss function & optimiser to use\n",
    "model_l2_AaN.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1236 samples, validate on 248 samples\n",
      "Epoch 1/15\n",
      "1236/1236 [==============================] - 9s 7ms/step - loss: 0.5467 - accuracy: 0.7759 - auc_2: 0.8008 - precision_2: 0.7641 - recall_2: 0.7641 - val_loss: 2.1227 - val_accuracy: 0.7863 - val_auc_2: 0.8126 - val_precision_2: 0.7760 - val_recall_2: 0.7760\n",
      "Epoch 2/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.3908 - accuracy: 0.8301 - auc_2: 0.8386 - precision_2: 0.7937 - recall_2: 0.7937 - val_loss: 0.4334 - val_accuracy: 0.8266 - val_auc_2: 0.8549 - val_precision_2: 0.8025 - val_recall_2: 0.8025\n",
      "Epoch 3/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.3095 - accuracy: 0.8762 - auc_2: 0.8723 - precision_2: 0.8178 - recall_2: 0.8178 - val_loss: 0.3950 - val_accuracy: 0.8306 - val_auc_2: 0.8824 - val_precision_2: 0.8248 - val_recall_2: 0.8248\n",
      "Epoch 4/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.2723 - accuracy: 0.8811 - auc_2: 0.8938 - precision_2: 0.8329 - recall_2: 0.8329 - val_loss: 0.7687 - val_accuracy: 0.6331 - val_auc_2: 0.8967 - val_precision_2: 0.8327 - val_recall_2: 0.8327\n",
      "Epoch 5/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.2356 - accuracy: 0.9005 - auc_2: 0.9018 - precision_2: 0.8354 - recall_2: 0.8354 - val_loss: 0.4776 - val_accuracy: 0.8145 - val_auc_2: 0.9076 - val_precision_2: 0.8404 - val_recall_2: 0.8404\n",
      "Epoch 6/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.2007 - accuracy: 0.9175 - auc_2: 0.9140 - precision_2: 0.8474 - recall_2: 0.8474 - val_loss: 0.5855 - val_accuracy: 0.7621 - val_auc_2: 0.9176 - val_precision_2: 0.8499 - val_recall_2: 0.8499\n",
      "Epoch 7/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.2522 - accuracy: 0.8908 - auc_2: 0.9203 - precision_2: 0.8519 - recall_2: 0.8519 - val_loss: 0.6031 - val_accuracy: 0.7500 - val_auc_2: 0.9214 - val_precision_2: 0.8526 - val_recall_2: 0.8526\n",
      "Epoch 8/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.2062 - accuracy: 0.9102 - auc_2: 0.9237 - precision_2: 0.8542 - recall_2: 0.8542 - val_loss: 0.4624 - val_accuracy: 0.8387 - val_auc_2: 0.9266 - val_precision_2: 0.8574 - val_recall_2: 0.8574\n",
      "Epoch 9/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.1596 - accuracy: 0.9337 - auc_2: 0.9300 - precision_2: 0.8611 - recall_2: 0.8611 - val_loss: 0.5278 - val_accuracy: 0.8347 - val_auc_2: 0.9325 - val_precision_2: 0.8642 - val_recall_2: 0.8642\n",
      "Epoch 10/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.1302 - accuracy: 0.9506 - auc_2: 0.9359 - precision_2: 0.8682 - recall_2: 0.8682 - val_loss: 0.5262 - val_accuracy: 0.8387 - val_auc_2: 0.9382 - val_precision_2: 0.8711 - val_recall_2: 0.8711\n",
      "Epoch 11/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.1171 - accuracy: 0.9579 - auc_2: 0.9409 - precision_2: 0.8745 - recall_2: 0.8745 - val_loss: 0.5290 - val_accuracy: 0.8347 - val_auc_2: 0.9426 - val_precision_2: 0.8771 - val_recall_2: 0.8771\n",
      "Epoch 12/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.1134 - accuracy: 0.9563 - auc_2: 0.9450 - precision_2: 0.8801 - recall_2: 0.8801 - val_loss: 0.8365 - val_accuracy: 0.8226 - val_auc_2: 0.9460 - val_precision_2: 0.8819 - val_recall_2: 0.8819\n",
      "Epoch 13/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.1420 - accuracy: 0.9482 - auc_2: 0.9473 - precision_2: 0.8840 - recall_2: 0.8840 - val_loss: 0.6191 - val_accuracy: 0.8387 - val_auc_2: 0.9482 - val_precision_2: 0.8856 - val_recall_2: 0.8856\n",
      "Epoch 14/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.1115 - accuracy: 0.9563 - auc_2: 0.9498 - precision_2: 0.8878 - recall_2: 0.8878 - val_loss: 0.5866 - val_accuracy: 0.8226 - val_auc_2: 0.9508 - val_precision_2: 0.8890 - val_recall_2: 0.8890\n",
      "Epoch 15/15\n",
      "1236/1236 [==============================] - 7s 6ms/step - loss: 0.0861 - accuracy: 0.9693 - auc_2: 0.9522 - precision_2: 0.8911 - recall_2: 0.8911 - val_loss: 0.6605 - val_accuracy: 0.8185 - val_auc_2: 0.9532 - val_precision_2: 0.8928 - val_recall_2: 0.8928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3f7205edd8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model_l2_AaN.fit(X_3d, y, epochs=15, batch_size=32, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in test data\n",
    "\n",
    "# Run through layer 1 classifier (rfc)\n",
    "\n",
    "# Run resulting class label 0s through to rfc_l2_NaO\n",
    "\n",
    "#Run resulting class label 1s through to rfc_l2_AaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X5990</th>\n",
       "      <th>X5991</th>\n",
       "      <th>X5992</th>\n",
       "      <th>X5993</th>\n",
       "      <th>X5994</th>\n",
       "      <th>X5995</th>\n",
       "      <th>X5996</th>\n",
       "      <th>X5997</th>\n",
       "      <th>X5998</th>\n",
       "      <th>X5999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00001</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00002</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.622</td>\n",
       "      <td>2.143</td>\n",
       "      <td>2.552</td>\n",
       "      <td>2.653</td>\n",
       "      <td>2.675</td>\n",
       "      <td>2.614</td>\n",
       "      <td>2.249</td>\n",
       "      <td>1.734</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.342</td>\n",
       "      <td>-1.354</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.376</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>-1.354</td>\n",
       "      <td>-1.342</td>\n",
       "      <td>-1.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00003</td>\n",
       "      <td>2.259</td>\n",
       "      <td>2.667</td>\n",
       "      <td>3.046</td>\n",
       "      <td>3.376</td>\n",
       "      <td>3.583</td>\n",
       "      <td>3.579</td>\n",
       "      <td>3.334</td>\n",
       "      <td>2.872</td>\n",
       "      <td>2.514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00004</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID     X0     X1     X2     X3     X4     X5     X6     X7     X8  ...  \\\n",
       "0  C00000 -0.169 -0.174 -0.184 -0.189 -0.200 -0.210 -0.221 -0.226 -0.226  ...   \n",
       "1  C00001 -0.184 -0.174 -0.169 -0.164 -0.158 -0.158 -0.158 -0.153 -0.153  ...   \n",
       "2  C00002  1.050  1.622  2.143  2.552  2.653  2.675  2.614  2.249  1.734  ...   \n",
       "3  C00003  2.259  2.667  3.046  3.376  3.583  3.579  3.334  2.872  2.514  ...   \n",
       "4  C00004 -0.203 -0.203 -0.201 -0.201 -0.201 -0.199 -0.194 -0.189 -0.185  ...   \n",
       "\n",
       "   X5990  X5991  X5992  X5993  X5994  X5995  X5996  X5997  X5998  X5999  \n",
       "0  0.375  0.386  0.386  0.360  0.323  0.282  0.240  0.184  0.132  0.090  \n",
       "1 -0.086 -0.117 -0.143 -0.148 -0.153 -0.153 -0.153 -0.153 -0.153 -0.158  \n",
       "2 -1.342 -1.354 -1.365 -1.370 -1.376 -1.370 -1.365 -1.354 -1.342 -1.326  \n",
       "3 -0.119 -0.119 -0.119 -0.116 -0.114 -0.109 -0.104 -0.097 -0.090 -0.083  \n",
       "4 -0.008  0.020  0.039  0.044  0.046  0.046  0.049  0.049  0.051  0.051  \n",
       "\n",
       "[5 rows x 6001 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data\n",
    "test_data = pd.read_csv('test_signal.csv')\n",
    "# Peek at the data\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 6000)\n",
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data from the 2nd feature column onwards are input\n",
    "test_X = test_data.values[:,1:]\n",
    "test_ids = test_data['ID']\n",
    "print(test_X.shape)\n",
    "print(test_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing data\n",
    "\n",
    "# Replace missing values with Nan\n",
    "test_X[test_X == ''] = np.nan\n",
    "\n",
    "# Replace Nan with median\n",
    "imputer = Imputer(missing_values=np.nan, strategy='mean')\n",
    "test_X = imputer.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input as 20 seconds of recording (for recurrent network)\n",
    "test_X_3d = test_X.reshape((4000, 20, 300))\n",
    "print(test_X_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95503104 0.04496894]\n",
      " [0.93077195 0.0692281 ]\n",
      " [0.71224684 0.28775322]\n",
      " ...\n",
      " [0.8888699  0.11113009]\n",
      " [0.83275557 0.16724446]\n",
      " [0.82662356 0.17337649]]\n"
     ]
    }
   ],
   "source": [
    "# Pass through 1st layer classifier\n",
    "predictions_ontest_first_layer_softmax = model_l1.predict(test_X_3d)\n",
    "print(predictions_ontest_first_layer_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Decode softmax\n",
    "predictions_ontest_first_layer = []\n",
    "for no_prob, an_prob in predictions_ontest_first_layer_softmax:\n",
    "    if no_prob > an_prob:\n",
    "        predictions_ontest_first_layer.append(0) # Normal & Other\n",
    "    else:\n",
    "        predictions_ontest_first_layer.append(1) # AF & Noisy\n",
    "\n",
    "print(predictions_ontest_first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>layer_1_class</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>...</th>\n",
       "      <th>X5990</th>\n",
       "      <th>X5991</th>\n",
       "      <th>X5992</th>\n",
       "      <th>X5993</th>\n",
       "      <th>X5994</th>\n",
       "      <th>X5995</th>\n",
       "      <th>X5996</th>\n",
       "      <th>X5997</th>\n",
       "      <th>X5998</th>\n",
       "      <th>X5999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00001</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00002</td>\n",
       "      <td>0</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.622</td>\n",
       "      <td>2.143</td>\n",
       "      <td>2.552</td>\n",
       "      <td>2.653</td>\n",
       "      <td>2.675</td>\n",
       "      <td>2.614</td>\n",
       "      <td>2.249</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.342</td>\n",
       "      <td>-1.354</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.376</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>-1.354</td>\n",
       "      <td>-1.342</td>\n",
       "      <td>-1.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00003</td>\n",
       "      <td>0</td>\n",
       "      <td>2.259</td>\n",
       "      <td>2.667</td>\n",
       "      <td>3.046</td>\n",
       "      <td>3.376</td>\n",
       "      <td>3.583</td>\n",
       "      <td>3.579</td>\n",
       "      <td>3.334</td>\n",
       "      <td>2.872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00004</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  layer_1_class     X0     X1     X2     X3     X4     X5     X6  \\\n",
       "0  C00000              0 -0.169 -0.174 -0.184 -0.189 -0.200 -0.210 -0.221   \n",
       "1  C00001              0 -0.184 -0.174 -0.169 -0.164 -0.158 -0.158 -0.158   \n",
       "2  C00002              0  1.050  1.622  2.143  2.552  2.653  2.675  2.614   \n",
       "3  C00003              0  2.259  2.667  3.046  3.376  3.583  3.579  3.334   \n",
       "4  C00004              0 -0.203 -0.203 -0.201 -0.201 -0.201 -0.199 -0.194   \n",
       "\n",
       "      X7  ...  X5990  X5991  X5992  X5993  X5994  X5995  X5996  X5997  X5998  \\\n",
       "0 -0.226  ...  0.375  0.386  0.386  0.360  0.323  0.282  0.240  0.184  0.132   \n",
       "1 -0.153  ... -0.086 -0.117 -0.143 -0.148 -0.153 -0.153 -0.153 -0.153 -0.153   \n",
       "2  2.249  ... -1.342 -1.354 -1.365 -1.370 -1.376 -1.370 -1.365 -1.354 -1.342   \n",
       "3  2.872  ... -0.119 -0.119 -0.119 -0.116 -0.114 -0.109 -0.104 -0.097 -0.090   \n",
       "4 -0.189  ... -0.008  0.020  0.039  0.044  0.046  0.046  0.049  0.049  0.051   \n",
       "\n",
       "   X5999  \n",
       "0  0.090  \n",
       "1 -0.158  \n",
       "2 -1.326  \n",
       "3 -0.083  \n",
       "4  0.051  \n",
       "\n",
       "[5 rows x 6002 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add predictions from the first layer NOT USED NOW WOULD DO THSI WHEN ACTUALLY TESTING IT\n",
    "# As column in the orginal training data\n",
    "test_data.insert(1, \"layer_1_class\", predictions_ontest_first_layer, True) \n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3983, 6002)\n",
      "(17, 6002)\n"
     ]
    }
   ],
   "source": [
    "# Split data set by result class label\n",
    "test_data_l2_NaO = test_data.loc[(test_data['layer_1_class'] == 0)]\n",
    "test_data_l2_AaN = test_data.loc[(test_data['layer_1_class'] == 1)]\n",
    "\n",
    "print(test_data_l2_NaO.shape)\n",
    "print(test_data_l2_AaN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3983, 6000)\n",
      "(3983,)\n",
      "(17, 6000)\n",
      "(17,)\n"
     ]
    }
   ],
   "source": [
    "# Input is after 1st and 2nd columns\n",
    "test_X_l2_NaO = test_data_l2_NaO.values[:,2:]\n",
    "#record IDs\n",
    "test_ids_l2_NaO = test_data_l2_NaO['ID']\n",
    "print(test_X_l2_NaO.shape)\n",
    "print(test_ids_l2_NaO.shape)\n",
    "\n",
    "# Input is after 1st and 2nd columns\n",
    "test_X_l2_AaN = test_data_l2_AaN.values[:,2:]\n",
    "#record IDs\n",
    "test_ids_l2_AaN = test_data_l2_AaN['ID']\n",
    "print(test_X_l2_AaN.shape)\n",
    "print(test_ids_l2_AaN.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing data\n",
    "\n",
    "# Replace missing values with Nan\n",
    "test_X_l2_NaO[test_X_l2_NaO == ''] = np.nan\n",
    "\n",
    "# Replace Nan with median\n",
    "imputer = Imputer(missing_values=np.nan, strategy='mean')\n",
    "test_X_l2_NaO = imputer.fit_transform(test_X_l2_NaO)\n",
    "\n",
    "# Replace missing values with Nan\n",
    "test_X_l2_AaN[test_X_l2_AaN == ''] = np.nan\n",
    "\n",
    "# Replace Nan with median\n",
    "imputer = Imputer(missing_values=np.nan, strategy='mean')\n",
    "test_X_l2_AaN = imputer.fit_transform(test_X_l2_AaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3983, 20, 300)\n",
      "(17, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input as 20 seconds of recording (for recurrent network)\n",
    "test_X_3d_l2_NaO = test_X_l2_NaO.reshape((3983, 20, 300))\n",
    "print(test_X_3d_l2_NaO.shape)\n",
    "\n",
    "test_X_3d_l2_AaN = test_X_l2_AaN.reshape((17, 20, 300))\n",
    "print(test_X_3d_l2_AaN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49217194 0.50782806]\n",
      " [0.45131597 0.548684  ]\n",
      " [0.14931075 0.8506893 ]\n",
      " ...\n",
      " [0.2671297  0.73287034]\n",
      " [0.23066898 0.76933104]\n",
      " [0.13461745 0.86538255]]\n"
     ]
    }
   ],
   "source": [
    "# Pass first subset through 2nd layer classifiers\n",
    "predictions_ontest_NaO_2nd_layer_softmax = model_l2_NaO.predict(test_X_3d_l2_NaO)\n",
    "print(predictions_ontest_NaO_2nd_layer_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0999278e-01 4.9000716e-01]\n",
      " [1.8720317e-03 9.9812800e-01]\n",
      " [6.1984356e-02 9.3801564e-01]\n",
      " [7.0587546e-01 2.9412448e-01]\n",
      " [5.1640192e-05 9.9994838e-01]\n",
      " [2.9437733e-01 7.0562273e-01]\n",
      " [4.4952169e-02 9.5504779e-01]\n",
      " [1.9351978e-02 9.8064798e-01]\n",
      " [8.9831448e-01 1.0168549e-01]\n",
      " [2.2587297e-03 9.9774134e-01]\n",
      " [2.9675793e-03 9.9703240e-01]\n",
      " [3.6111072e-01 6.3888931e-01]\n",
      " [8.8024430e-02 9.1197562e-01]\n",
      " [4.7244900e-01 5.2755100e-01]\n",
      " [5.2208509e-02 9.4779152e-01]\n",
      " [2.4226280e-03 9.9757737e-01]\n",
      " [9.4154787e-01 5.8452085e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Pass second subset through 2nd layer classifiers\n",
    "predictions_ontest_AaN_2nd_layer_softmax = model_l2_AaN.predict(test_X_3d_l2_AaN)\n",
    "print(predictions_ontest_AaN_2nd_layer_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#Decode first subset's softmax\n",
    "predictions_ontest_NaO_2nd_layer = []\n",
    "for o_prob, n_prob in predictions_ontest_NaO_2nd_layer_softmax:\n",
    "    if o_prob > n_prob:\n",
    "        predictions_ontest_NaO_2nd_layer.append(0) # Other\n",
    "    else:\n",
    "        predictions_ontest_NaO_2nd_layer.append(1) # Normal\n",
    "\n",
    "print(predictions_ontest_NaO_2nd_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#Decode second subset's softmax\n",
    "predictions_ontest_AaN_2nd_layer = []\n",
    "for n_prob, a_prob in predictions_ontest_AaN_2nd_layer_softmax:\n",
    "    if n_prob > a_prob:\n",
    "        predictions_ontest_AaN_2nd_layer.append(0) # Noise\n",
    "    else:\n",
    "        predictions_ontest_AaN_2nd_layer.append(1) # Af\n",
    "\n",
    "print(predictions_ontest_AaN_2nd_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C00000', 1), ('C00001', 1), ('C00002', 1), ('C00003', 1), ('C00004', 1), ('C00005', 1), ('C00006', 1), ('C00007', 1), ('C00008', 1), ('C00009', 1), ('C00010', 1), ('C00011', 1), ('C00012', 1), ('C00013', 1), ('C00014', 1), ('C00015', 1), ('C00016', 1), ('C00017', 1), ('C00018', 1), ('C00019', 1), ('C00020', 1), ('C00021', 0), ('C00022', 0), ('C00023', 1), ('C00024', 1), ('C00025', 1), ('C00026', 1), ('C00027', 1), ('C00028', 1), ('C00029', 1), ('C00030', 1), ('C00031', 1), ('C00032', 1), ('C00033', 1), ('C00034', 1), ('C00035', 1), ('C00036', 1), ('C00037', 1), ('C00038', 1), ('C00039', 1), ('C00040', 1), ('C00041', 1), ('C00042', 1), ('C00043', 0), ('C00044', 0), ('C00045', 1), ('C00046', 1), ('C00047', 1), ('C00049', 1), ('C00050', 1), ('C00051', 1), ('C00052', 1), ('C00053', 1), ('C00054', 1), ('C00055', 1), ('C00056', 1), ('C00057', 1), ('C00058', 1), ('C00059', 1), ('C00060', 1), ('C00061', 1), ('C00062', 1), ('C00063', 1), ('C00064', 1), ('C00065', 1), ('C00066', 1), ('C00067', 1), ('C00068', 0), ('C00069', 1), ('C00070', 1), ('C00071', 1), ('C00072', 1), ('C00073', 1), ('C00074', 1), ('C00075', 1), ('C00076', 1), ('C00077', 1), ('C00078', 1), ('C00079', 1), ('C00080', 1), ('C00081', 1), ('C00082', 1), ('C00083', 1), ('C00084', 1), ('C00085', 1), ('C00086', 1), ('C00087', 1), ('C00088', 1), ('C00089', 1), ('C00090', 1), ('C00091', 1), ('C00092', 1), ('C00093', 1), ('C00094', 1), ('C00095', 1), ('C00096', 1), ('C00097', 1), ('C00098', 1), ('C00099', 1), ('C00100', 1), ('C00101', 1), ('C00102', 1), ('C00103', 1), ('C00104', 1), ('C00105', 1), ('C00106', 0), ('C00107', 1), ('C00108', 1), ('C00109', 1), ('C00110', 1), ('C00111', 1), ('C00112', 1), ('C00113', 1), ('C00114', 1), ('C00115', 1), ('C00116', 1), ('C00117', 1), ('C00118', 0), ('C00119', 1), ('C00120', 1), ('C00121', 1), ('C00122', 0), ('C00123', 1), ('C00124', 1), ('C00125', 1), ('C00126', 1), ('C00127', 1), ('C00128', 1), ('C00129', 1), ('C00130', 1), ('C00131', 1), ('C00132', 1), ('C00133', 1), ('C00134', 1), ('C00135', 1), ('C00136', 1), ('C00137', 1), ('C00138', 1), ('C00139', 1), ('C00140', 1), ('C00141', 1), ('C00142', 1), ('C00143', 1), ('C00144', 1), ('C00145', 1), ('C00146', 1), ('C00147', 1), ('C00148', 1), ('C00149', 1), ('C00150', 1), ('C00151', 1), ('C00152', 1), ('C00153', 1), ('C00154', 1), ('C00155', 1), ('C00156', 1), ('C00157', 1), ('C00158', 0), ('C00159', 1), ('C00160', 1), ('C00161', 1), ('C00162', 1), ('C00163', 1), ('C00164', 1), ('C00165', 1), ('C00166', 1), ('C00167', 0), ('C00168', 0), ('C00169', 1), ('C00170', 1), ('C00171', 1), ('C00172', 0), ('C00173', 1), ('C00174', 0), ('C00175', 1), ('C00176', 1), ('C00177', 1), ('C00178', 1), ('C00179', 1), ('C00180', 1), ('C00181', 0), ('C00182', 1), ('C00183', 1), ('C00184', 1), ('C00185', 1), ('C00186', 1), ('C00187', 1), ('C00188', 1), ('C00189', 1), ('C00190', 1), ('C00191', 0), ('C00192', 1), ('C00193', 1), ('C00194', 1), ('C00195', 1), ('C00196', 1), ('C00197', 1), ('C00198', 1), ('C00199', 1), ('C00200', 1), ('C00201', 1), ('C00202', 1), ('C00203', 1), ('C00204', 1), ('C00205', 1), ('C00206', 1), ('C00207', 1), ('C00208', 1), ('C00209', 1), ('C00210', 1), ('C00211', 1), ('C00212', 1), ('C00213', 1), ('C00214', 1), ('C00215', 1), ('C00216', 1), ('C00217', 1), ('C00218', 1), ('C00219', 1), ('C00220', 1), ('C00221', 1), ('C00222', 1), ('C00223', 1), ('C00224', 1), ('C00225', 1), ('C00226', 1), ('C00227', 1), ('C00228', 1), ('C00229', 1), ('C00230', 1), ('C00231', 1), ('C00232', 1), ('C00233', 1), ('C00234', 1), ('C00235', 1), ('C00236', 1), ('C00237', 1), ('C00238', 1), ('C00240', 1), ('C00241', 1), ('C00242', 0), ('C00243', 1), ('C00244', 1), ('C00245', 1), ('C00246', 1), ('C00247', 1), ('C00248', 1), ('C00249', 1), ('C00250', 1), ('C00251', 1), ('C00252', 1), ('C00253', 1), ('C00254', 1), ('C00255', 1), ('C00256', 1), ('C00257', 1), ('C00258', 1), ('C00259', 1), ('C00260', 1), ('C00261', 1), ('C00262', 1), ('C00263', 1), ('C00264', 1), ('C00265', 1), ('C00266', 1), ('C00267', 1), ('C00268', 1), ('C00269', 1), ('C00270', 1), ('C00271', 1), ('C00272', 1), ('C00273', 1), ('C00274', 1), ('C00275', 1), ('C00276', 1), ('C00277', 1), ('C00278', 1), ('C00279', 1), ('C00280', 1), ('C00281', 1), ('C00282', 1), ('C00283', 1), ('C00284', 0), ('C00285', 0), ('C00286', 1), ('C00287', 1), ('C00288', 1), ('C00289', 1), ('C00290', 1), ('C00291', 1), ('C00292', 1), ('C00293', 1), ('C00294', 1), ('C00295', 1), ('C00296', 1), ('C00297', 0), ('C00298', 1), ('C00299', 1), ('C00300', 1), ('C00301', 1), ('C00302', 1), ('C00303', 1), ('C00304', 1), ('C00305', 1), ('C00306', 1), ('C00307', 1), ('C00308', 1), ('C00309', 1), ('C00310', 0), ('C00311', 1), ('C00312', 1), ('C00313', 0), ('C00314', 1), ('C00315', 1), ('C00316', 1), ('C00317', 1), ('C00318', 1), ('C00319', 1), ('C00320', 1), ('C00321', 1), ('C00322', 1), ('C00323', 1), ('C00324', 0), ('C00325', 1), ('C00326', 1), ('C00327', 1), ('C00328', 0), ('C00329', 1), ('C00330', 1), ('C00331', 1), ('C00332', 1), ('C00333', 1), ('C00334', 1), ('C00335', 1), ('C00336', 1), ('C00337', 1), ('C00338', 1), ('C00339', 1), ('C00340', 1), ('C00341', 1), ('C00342', 1), ('C00343', 1), ('C00344', 0), ('C00345', 1), ('C00346', 0), ('C00347', 1), ('C00348', 1), ('C00349', 1), ('C00350', 1), ('C00351', 1), ('C00352', 1), ('C00353', 1), ('C00354', 0), ('C00355', 1), ('C00356', 1), ('C00357', 1), ('C00358', 1), ('C00359', 1), ('C00360', 1), ('C00361', 0), ('C00362', 1), ('C00363', 1), ('C00364', 1), ('C00365', 1), ('C00366', 1), ('C00367', 0), ('C00368', 1), ('C00369', 1), ('C00370', 1), ('C00371', 1), ('C00372', 1), ('C00373', 1), ('C00374', 1), ('C00375', 1), ('C00376', 1), ('C00377', 1), ('C00378', 1), ('C00379', 1), ('C00380', 1), ('C00381', 1), ('C00382', 1), ('C00383', 1), ('C00384', 1), ('C00385', 1), ('C00386', 1), ('C00387', 1), ('C00388', 1), ('C00389', 1), ('C00390', 0), ('C00391', 1), ('C00392', 1), ('C00393', 1), ('C00394', 1), ('C00395', 1), ('C00396', 1), ('C00397', 1), ('C00398', 1), ('C00399', 1), ('C00400', 1), ('C00401', 1), ('C00402', 1), ('C00403', 1), ('C00404', 1), ('C00405', 1), ('C00406', 1), ('C00407', 1), ('C00408', 1), ('C00409', 1), ('C00410', 1), ('C00411', 1), ('C00412', 1), ('C00413', 0), ('C00414', 1), ('C00415', 1), ('C00416', 1), ('C00417', 1), ('C00418', 1), ('C00419', 1), ('C00420', 1), ('C00421', 1), ('C00422', 1), ('C00423', 1), ('C00424', 1), ('C00425', 1), ('C00426', 1), ('C00427', 1), ('C00428', 1), ('C00429', 1), ('C00430', 1), ('C00431', 1), ('C00432', 1), ('C00433', 1), ('C00434', 1), ('C00435', 1), ('C00436', 1), ('C00437', 0), ('C00438', 1), ('C00439', 1), ('C00440', 1), ('C00441', 1), ('C00442', 1), ('C00443', 1), ('C00444', 1), ('C00445', 1), ('C00446', 1), ('C00447', 1), ('C00448', 1), ('C00449', 1), ('C00450', 1), ('C00451', 0), ('C00452', 1), ('C00453', 1), ('C00454', 1), ('C00455', 1), ('C00456', 1), ('C00457', 1), ('C00458', 1), ('C00459', 1), ('C00460', 1), ('C00461', 1), ('C00462', 1), ('C00463', 1), ('C00464', 1), ('C00465', 1), ('C00466', 1), ('C00467', 1), ('C00468', 1), ('C00469', 1), ('C00470', 1), ('C00471', 1), ('C00472', 1), ('C00473', 1), ('C00474', 1), ('C00475', 1), ('C00476', 1), ('C00477', 1), ('C00478', 1), ('C00479', 1), ('C00480', 1), ('C00481', 1), ('C00482', 1), ('C00483', 1), ('C00484', 1), ('C00485', 1), ('C00486', 0), ('C00487', 1), ('C00488', 0), ('C00489', 1), ('C00490', 1), ('C00491', 1), ('C00492', 1), ('C00493', 1), ('C00494', 1), ('C00495', 1), ('C00496', 1), ('C00497', 1), ('C00498', 1), ('C00499', 1), ('C00500', 1), ('C00501', 1), ('C00502', 1), ('C00503', 1), ('C00504', 1), ('C00505', 1), ('C00506', 1), ('C00507', 1), ('C00508', 0), ('C00509', 1), ('C00510', 1), ('C00511', 0), ('C00512', 0), ('C00513', 1), ('C00514', 1), ('C00515', 1), ('C00516', 1), ('C00517', 1), ('C00518', 1), ('C00519', 1), ('C00520', 1), ('C00521', 1), ('C00522', 1), ('C00523', 1), ('C00524', 1), ('C00525', 1), ('C00526', 1), ('C00527', 1), ('C00528', 1), ('C00529', 1), ('C00530', 1), ('C00531', 1), ('C00532', 1), ('C00533', 1), ('C00534', 1), ('C00535', 1), ('C00536', 1), ('C00537', 1), ('C00538', 1), ('C00539', 1), ('C00540', 1), ('C00541', 1), ('C00542', 1), ('C00543', 1), ('C00544', 1), ('C00545', 1), ('C00546', 1), ('C00547', 1), ('C00548', 1), ('C00549', 1), ('C00550', 1), ('C00551', 1), ('C00552', 0), ('C00553', 1), ('C00554', 1), ('C00555', 1), ('C00556', 1), ('C00557', 1), ('C00558', 1), ('C00559', 1), ('C00560', 1), ('C00561', 1), ('C00562', 1), ('C00563', 1), ('C00564', 1), ('C00565', 1), ('C00566', 1), ('C00567', 1), ('C00568', 1), ('C00569', 1), ('C00570', 1), ('C00571', 1), ('C00572', 1), ('C00573', 1), ('C00574', 1), ('C00575', 1), ('C00576', 1), ('C00577', 1), ('C00578', 1), ('C00579', 1), ('C00580', 1), ('C00581', 1), ('C00582', 1), ('C00583', 1), ('C00584', 1), ('C00585', 1), ('C00586', 1), ('C00587', 1), ('C00588', 1), ('C00589', 1), ('C00590', 1), ('C00591', 1), ('C00592', 1), ('C00593', 1), ('C00594', 1), ('C00595', 1), ('C00596', 1), ('C00597', 1), ('C00598', 1), ('C00599', 1), ('C00600', 1), ('C00601', 1), ('C00602', 1), ('C00603', 1), ('C00604', 1), ('C00606', 1), ('C00607', 1), ('C00608', 1), ('C00609', 1), ('C00610', 1), ('C00611', 1), ('C00612', 1), ('C00613', 0), ('C00614', 1), ('C00615', 1), ('C00616', 1), ('C00617', 1), ('C00618', 1), ('C00619', 1), ('C00620', 1), ('C00621', 1), ('C00622', 1), ('C00623', 1), ('C00624', 1), ('C00625', 1), ('C00626', 1), ('C00627', 1), ('C00628', 1), ('C00629', 1), ('C00630', 1), ('C00631', 1), ('C00632', 1), ('C00633', 1), ('C00634', 1), ('C00635', 1), ('C00636', 1), ('C00637', 1), ('C00638', 1), ('C00639', 1), ('C00640', 1), ('C00641', 1), ('C00642', 1), ('C00643', 1), ('C00644', 1), ('C00645', 1), ('C00646', 1), ('C00647', 1), ('C00648', 1), ('C00649', 1), ('C00650', 1), ('C00651', 1), ('C00652', 1), ('C00653', 1), ('C00654', 1), ('C00655', 1), ('C00656', 1), ('C00657', 1), ('C00658', 1), ('C00659', 1), ('C00660', 1), ('C00661', 1), ('C00662', 1), ('C00663', 1), ('C00664', 1), ('C00665', 1), ('C00666', 1), ('C00667', 1), ('C00668', 1), ('C00669', 1), ('C00670', 1), ('C00671', 1), ('C00672', 1), ('C00673', 1), ('C00674', 1), ('C00675', 1), ('C00676', 1), ('C00677', 1), ('C00678', 1), ('C00679', 1), ('C00680', 0), ('C00681', 0), ('C00682', 1), ('C00683', 1), ('C00684', 1), ('C00685', 1), ('C00686', 1), ('C00687', 1), ('C00688', 1), ('C00689', 1), ('C00690', 0), ('C00691', 1), ('C00692', 1), ('C00693', 1), ('C00694', 1), ('C00695', 0), ('C00696', 1), ('C00697', 1), ('C00698', 1), ('C00699', 1), ('C00700', 1), ('C00701', 1), ('C00702', 0), ('C00703', 1), ('C00704', 1), ('C00705', 1), ('C00706', 1), ('C00707', 1), ('C00708', 1), ('C00709', 1), ('C00710', 1), ('C00711', 1), ('C00712', 1), ('C00713', 1), ('C00714', 1), ('C00715', 1), ('C00716', 1), ('C00717', 1), ('C00718', 1), ('C00719', 1), ('C00720', 1), ('C00721', 1), ('C00722', 1), ('C00723', 1), ('C00724', 1), ('C00725', 1), ('C00726', 1), ('C00727', 1), ('C00728', 1), ('C00729', 1), ('C00730', 1), ('C00731', 1), ('C00732', 1), ('C00733', 1), ('C00734', 1), ('C00735', 1), ('C00736', 1), ('C00737', 1), ('C00738', 1), ('C00739', 1), ('C00740', 1), ('C00741', 1), ('C00742', 1), ('C00743', 1), ('C00744', 1), ('C00745', 1), ('C00746', 1), ('C00747', 1), ('C00748', 1), ('C00749', 1), ('C00750', 1), ('C00751', 1), ('C00752', 1), ('C00753', 1), ('C00754', 1), ('C00755', 1), ('C00756', 1), ('C00757', 1), ('C00758', 1), ('C00759', 1), ('C00760', 1), ('C00761', 1), ('C00762', 1), ('C00763', 1), ('C00764', 1), ('C00765', 1), ('C00766', 1), ('C00767', 1), ('C00768', 1), ('C00769', 1), ('C00770', 1), ('C00771', 1), ('C00772', 1), ('C00773', 1), ('C00774', 1), ('C00775', 1), ('C00776', 1), ('C00777', 1), ('C00778', 0), ('C00779', 1), ('C00780', 1), ('C00781', 1), ('C00782', 1), ('C00783', 1), ('C00784', 1), ('C00785', 1), ('C00786', 1), ('C00787', 0), ('C00788', 0), ('C00789', 0), ('C00790', 1), ('C00791', 1), ('C00792', 1), ('C00793', 1), ('C00794', 1), ('C00795', 1), ('C00797', 1), ('C00798', 1), ('C00799', 1), ('C00800', 1), ('C00801', 1), ('C00802', 1), ('C00803', 1), ('C00804', 1), ('C00805', 1), ('C00806', 1), ('C00807', 1), ('C00808', 0), ('C00809', 1), ('C00811', 1), ('C00812', 1), ('C00813', 1), ('C00814', 1), ('C00815', 1), ('C00816', 0), ('C00817', 1), ('C00818', 1), ('C00819', 0), ('C00820', 1), ('C00821', 1), ('C00822', 0), ('C00823', 1), ('C00824', 1), ('C00825', 1), ('C00826', 1), ('C00827', 1), ('C00828', 1), ('C00829', 1), ('C00830', 1), ('C00831', 1), ('C00832', 1), ('C00833', 0), ('C00834', 1), ('C00835', 1), ('C00836', 1), ('C00838', 1), ('C00839', 1), ('C00840', 0), ('C00841', 1), ('C00842', 1), ('C00843', 1), ('C00844', 1), ('C00845', 1), ('C00846', 1), ('C00847', 0), ('C00848', 1), ('C00849', 1), ('C00850', 1), ('C00851', 1), ('C00852', 1), ('C00853', 1), ('C00854', 1), ('C00855', 1), ('C00856', 1), ('C00857', 1), ('C00858', 1), ('C00859', 1), ('C00860', 1), ('C00861', 1), ('C00862', 1), ('C00863', 1), ('C00864', 1), ('C00865', 1), ('C00866', 1), ('C00867', 1), ('C00868', 1), ('C00869', 1), ('C00870', 1), ('C00871', 1), ('C00872', 1), ('C00873', 1), ('C00874', 1), ('C00875', 1), ('C00876', 1), ('C00877', 1), ('C00878', 1), ('C00879', 1), ('C00880', 1), ('C00881', 1), ('C00882', 1), ('C00883', 1), ('C00884', 1), ('C00885', 1), ('C00886', 0), ('C00887', 1), ('C00888', 0), ('C00889', 1), ('C00890', 1), ('C00891', 0), ('C00892', 1), ('C00893', 1), ('C00894', 1), ('C00895', 0), ('C00896', 1), ('C00897', 1), ('C00898', 1), ('C00899', 1), ('C00900', 1), ('C00901', 1), ('C00902', 1), ('C00903', 1), ('C00904', 1), ('C00905', 1), ('C00906', 1), ('C00907', 1), ('C00908', 1), ('C00909', 1), ('C00910', 1), ('C00911', 1), ('C00912', 1), ('C00913', 1), ('C00914', 1), ('C00915', 1), ('C00916', 1), ('C00917', 1), ('C00918', 1), ('C00919', 1), ('C00920', 1), ('C00921', 1), ('C00922', 1), ('C00923', 1), ('C00924', 1), ('C00925', 1), ('C00926', 1), ('C00927', 1), ('C00928', 1), ('C00929', 1), ('C00930', 1), ('C00931', 0), ('C00932', 1), ('C00933', 1), ('C00934', 1), ('C00935', 1), ('C00936', 1), ('C00937', 1), ('C00938', 1), ('C00939', 1), ('C00940', 1), ('C00941', 0), ('C00942', 1), ('C00943', 1), ('C00944', 1), ('C00945', 1), ('C00946', 1), ('C00947', 1), ('C00948', 1), ('C00949', 1), ('C00950', 1), ('C00951', 1), ('C00952', 1), ('C00953', 1), ('C00954', 1), ('C00955', 1), ('C00956', 1), ('C00957', 0), ('C00958', 1), ('C00959', 1), ('C00960', 1), ('C00961', 1), ('C00962', 1), ('C00963', 1), ('C00964', 1), ('C00965', 1), ('C00966', 1), ('C00967', 1), ('C00968', 1), ('C00969', 1), ('C00970', 1), ('C00971', 1), ('C00972', 1), ('C00973', 1), ('C00974', 1), ('C00975', 1), ('C00976', 1), ('C00977', 1), ('C00978', 1), ('C00979', 1), ('C00980', 1), ('C00981', 1), ('C00982', 1), ('C00983', 1), ('C00984', 1), ('C00985', 1), ('C00986', 1), ('C00987', 1), ('C00988', 1), ('C00989', 1), ('C00990', 1), ('C00991', 1), ('C00992', 0), ('C00993', 0), ('C00994', 1), ('C00995', 1), ('C00996', 1), ('C00997', 1), ('C00998', 1), ('C00999', 1), ('C01000', 1), ('C01001', 1), ('C01002', 1), ('C01003', 1), ('C01004', 1), ('C01005', 1), ('C01006', 1), ('C01007', 1), ('C01008', 1), ('C01009', 1), ('C01010', 1), ('C01011', 1), ('C01012', 1), ('C01013', 1), ('C01014', 1), ('C01015', 1), ('C01016', 1), ('C01017', 1), ('C01018', 1), ('C01019', 1), ('C01020', 0), ('C01021', 0), ('C01022', 1), ('C01023', 0), ('C01024', 1), ('C01025', 1), ('C01026', 1), ('C01027', 1), ('C01028', 1), ('C01029', 1), ('C01030', 1), ('C01031', 1), ('C01032', 1), ('C01033', 0), ('C01034', 1), ('C01035', 1), ('C01036', 1), ('C01037', 1), ('C01038', 1), ('C01039', 1), ('C01040', 1), ('C01041', 1), ('C01042', 1), ('C01043', 1), ('C01044', 1), ('C01045', 1), ('C01046', 1), ('C01047', 1), ('C01048', 1), ('C01049', 1), ('C01050', 1), ('C01051', 1), ('C01052', 1), ('C01053', 1), ('C01054', 1), ('C01055', 1), ('C01056', 1), ('C01057', 1), ('C01058', 1), ('C01059', 1), ('C01060', 1), ('C01061', 0), ('C01062', 1), ('C01063', 1), ('C01064', 1), ('C01065', 1), ('C01066', 1), ('C01067', 1), ('C01068', 1), ('C01069', 1), ('C01070', 1), ('C01071', 1), ('C01072', 1), ('C01073', 1), ('C01074', 1), ('C01075', 1), ('C01076', 0), ('C01077', 1), ('C01078', 1), ('C01079', 1), ('C01080', 1), ('C01081', 1), ('C01082', 1), ('C01083', 1), ('C01084', 1), ('C01085', 1), ('C01086', 1), ('C01087', 1), ('C01088', 0), ('C01089', 0), ('C01090', 1), ('C01091', 1), ('C01092', 1), ('C01093', 1), ('C01094', 0), ('C01095', 1), ('C01096', 1), ('C01097', 1), ('C01098', 1), ('C01099', 1), ('C01100', 1), ('C01101', 1), ('C01102', 1), ('C01103', 1), ('C01104', 1), ('C01105', 1), ('C01106', 1), ('C01107', 1), ('C01108', 1), ('C01109', 1), ('C01110', 1), ('C01111', 1), ('C01112', 1), ('C01113', 0), ('C01114', 1), ('C01115', 1), ('C01116', 1), ('C01117', 1), ('C01118', 1), ('C01119', 1), ('C01120', 1), ('C01121', 0), ('C01122', 1), ('C01123', 1), ('C01124', 1), ('C01125', 1), ('C01126', 1), ('C01127', 1), ('C01128', 1), ('C01129', 1), ('C01130', 1), ('C01131', 1), ('C01132', 1), ('C01133', 1), ('C01134', 1), ('C01135', 1), ('C01136', 1), ('C01137', 1), ('C01138', 1), ('C01139', 1), ('C01140', 1), ('C01141', 0), ('C01142', 1), ('C01143', 1), ('C01144', 1), ('C01145', 1), ('C01146', 1), ('C01147', 1), ('C01148', 1), ('C01149', 1), ('C01150', 1), ('C01151', 1), ('C01152', 1), ('C01153', 1), ('C01154', 1), ('C01155', 1), ('C01157', 1), ('C01158', 1), ('C01159', 1), ('C01160', 1), ('C01161', 1), ('C01162', 1), ('C01163', 1), ('C01164', 1), ('C01165', 1), ('C01166', 1), ('C01167', 1), ('C01168', 1), ('C01169', 1), ('C01170', 1), ('C01171', 1), ('C01172', 1), ('C01173', 1), ('C01174', 1), ('C01175', 1), ('C01176', 0), ('C01177', 1), ('C01178', 1), ('C01179', 1), ('C01180', 1), ('C01181', 1), ('C01182', 1), ('C01183', 1), ('C01184', 1), ('C01185', 1), ('C01186', 1), ('C01187', 1), ('C01188', 1), ('C01189', 1), ('C01190', 1), ('C01191', 1), ('C01192', 1), ('C01193', 1), ('C01194', 1), ('C01195', 1), ('C01196', 1), ('C01197', 1), ('C01198', 1), ('C01199', 1), ('C01200', 1), ('C01201', 1), ('C01202', 1), ('C01203', 1), ('C01204', 1), ('C01205', 1), ('C01206', 1), ('C01207', 1), ('C01208', 1), ('C01209', 1), ('C01210', 1), ('C01211', 1), ('C01212', 1), ('C01213', 1), ('C01214', 1), ('C01215', 1), ('C01216', 1), ('C01217', 1), ('C01218', 1), ('C01219', 1), ('C01220', 1), ('C01221', 1), ('C01222', 1), ('C01223', 1), ('C01224', 1), ('C01225', 1), ('C01226', 1), ('C01227', 1), ('C01228', 1), ('C01229', 1), ('C01230', 1), ('C01231', 1), ('C01232', 1), ('C01233', 1), ('C01234', 1), ('C01235', 1), ('C01236', 1), ('C01237', 1), ('C01238', 1), ('C01239', 1), ('C01240', 1), ('C01241', 1), ('C01242', 1), ('C01243', 1), ('C01244', 1), ('C01245', 0), ('C01246', 1), ('C01247', 1), ('C01248', 1), ('C01249', 1), ('C01250', 1), ('C01251', 1), ('C01252', 1), ('C01253', 1), ('C01254', 1), ('C01255', 1), ('C01256', 1), ('C01257', 1), ('C01258', 1), ('C01259', 1), ('C01260', 1), ('C01261', 1), ('C01262', 1), ('C01263', 1), ('C01264', 1), ('C01265', 1), ('C01266', 1), ('C01267', 1), ('C01268', 0), ('C01269', 1), ('C01270', 1), ('C01271', 1), ('C01272', 1), ('C01273', 0), ('C01274', 1), ('C01275', 1), ('C01276', 1), ('C01277', 0), ('C01278', 1), ('C01279', 1), ('C01280', 1), ('C01281', 1), ('C01282', 1), ('C01283', 1), ('C01284', 0), ('C01285', 1), ('C01286', 1), ('C01287', 1), ('C01288', 1), ('C01289', 1), ('C01290', 1), ('C01291', 1), ('C01292', 1), ('C01293', 0), ('C01294', 1), ('C01295', 1), ('C01296', 1), ('C01297', 1), ('C01299', 1), ('C01300', 1), ('C01301', 1), ('C01302', 1), ('C01303', 1), ('C01304', 1), ('C01305', 1), ('C01306', 1), ('C01307', 1), ('C01308', 1), ('C01309', 1), ('C01310', 1), ('C01311', 1), ('C01312', 1), ('C01313', 1), ('C01314', 1), ('C01315', 1), ('C01316', 1), ('C01317', 1), ('C01318', 0), ('C01319', 1), ('C01320', 1), ('C01321', 0), ('C01322', 1), ('C01323', 1), ('C01324', 1), ('C01325', 1), ('C01326', 1), ('C01327', 1), ('C01328', 0), ('C01329', 0), ('C01330', 1), ('C01331', 1), ('C01332', 1), ('C01333', 1), ('C01334', 1), ('C01335', 1), ('C01336', 1), ('C01337', 1), ('C01338', 1), ('C01339', 1), ('C01341', 1), ('C01342', 1), ('C01343', 1), ('C01344', 1), ('C01345', 1), ('C01346', 1), ('C01347', 1), ('C01348', 1), ('C01349', 1), ('C01350', 1), ('C01351', 1), ('C01352', 1), ('C01353', 1), ('C01354', 1), ('C01355', 1), ('C01356', 0), ('C01357', 1), ('C01358', 1), ('C01359', 1), ('C01360', 1), ('C01361', 1), ('C01362', 1), ('C01363', 1), ('C01364', 1), ('C01365', 1), ('C01366', 1), ('C01367', 1), ('C01368', 1), ('C01369', 1), ('C01370', 1), ('C01371', 1), ('C01372', 1), ('C01373', 1), ('C01374', 1), ('C01375', 1), ('C01376', 1), ('C01377', 1), ('C01378', 0), ('C01379', 1), ('C01380', 1), ('C01381', 1), ('C01382', 1), ('C01383', 1), ('C01384', 1), ('C01385', 1), ('C01386', 1), ('C01387', 1), ('C01388', 1), ('C01389', 1), ('C01390', 1), ('C01391', 1), ('C01392', 1), ('C01393', 1), ('C01394', 1), ('C01395', 1), ('C01396', 0), ('C01397', 1), ('C01398', 1), ('C01399', 1), ('C01400', 1), ('C01401', 1), ('C01402', 1), ('C01403', 1), ('C01404', 1), ('C01405', 1), ('C01406', 1), ('C01407', 1), ('C01408', 1), ('C01409', 1), ('C01410', 1), ('C01411', 1), ('C01412', 1), ('C01413', 1), ('C01414', 1), ('C01415', 1), ('C01416', 1), ('C01417', 1), ('C01418', 1), ('C01419', 1), ('C01420', 0), ('C01421', 1), ('C01422', 1), ('C01423', 1), ('C01424', 1), ('C01425', 0), ('C01426', 1), ('C01427', 1), ('C01428', 1), ('C01429', 1), ('C01430', 1), ('C01431', 1), ('C01432', 1), ('C01433', 1), ('C01434', 1), ('C01435', 1), ('C01436', 1), ('C01437', 1), ('C01438', 1), ('C01439', 1), ('C01440', 1), ('C01441', 1), ('C01442', 1), ('C01443', 1), ('C01444', 1), ('C01445', 1), ('C01446', 1), ('C01447', 1), ('C01448', 1), ('C01449', 1), ('C01450', 1), ('C01451', 1), ('C01452', 1), ('C01453', 1), ('C01454', 1), ('C01455', 1), ('C01456', 0), ('C01457', 1), ('C01458', 1), ('C01459', 1), ('C01460', 1), ('C01461', 1), ('C01462', 1), ('C01463', 1), ('C01464', 1), ('C01465', 0), ('C01466', 1), ('C01467', 1), ('C01468', 1), ('C01469', 1), ('C01470', 1), ('C01471', 1), ('C01472', 1), ('C01473', 1), ('C01474', 1), ('C01475', 1), ('C01476', 1), ('C01478', 1), ('C01479', 1), ('C01480', 1), ('C01481', 1), ('C01482', 1), ('C01483', 1), ('C01484', 1), ('C01485', 1), ('C01486', 1), ('C01487', 1), ('C01488', 1), ('C01489', 1), ('C01490', 1), ('C01491', 1), ('C01492', 1), ('C01493', 1), ('C01494', 1), ('C01495', 1), ('C01496', 1), ('C01497', 1), ('C01498', 1), ('C01499', 1), ('C01500', 1), ('C01501', 1), ('C01502', 1), ('C01503', 1), ('C01504', 1), ('C01505', 1), ('C01506', 1), ('C01507', 1), ('C01508', 1), ('C01509', 1), ('C01510', 1), ('C01511', 1), ('C01512', 1), ('C01513', 1), ('C01514', 1), ('C01515', 1), ('C01516', 1), ('C01517', 0), ('C01518', 1), ('C01519', 0), ('C01520', 1), ('C01521', 1), ('C01522', 1), ('C01523', 1), ('C01524', 1), ('C01525', 1), ('C01526', 1), ('C01527', 1), ('C01528', 1), ('C01529', 1), ('C01530', 1), ('C01531', 0), ('C01532', 1), ('C01533', 1), ('C01534', 1), ('C01535', 1), ('C01536', 1), ('C01537', 1), ('C01538', 1), ('C01539', 1), ('C01540', 1), ('C01541', 1), ('C01542', 1), ('C01543', 1), ('C01544', 1), ('C01545', 1), ('C01546', 1), ('C01547', 1), ('C01548', 1), ('C01549', 1), ('C01550', 1), ('C01551', 1), ('C01552', 1), ('C01553', 1), ('C01554', 1), ('C01555', 1), ('C01556', 1), ('C01557', 0), ('C01558', 1), ('C01559', 1), ('C01560', 1), ('C01561', 1), ('C01562', 1), ('C01563', 1), ('C01564', 1), ('C01565', 1), ('C01566', 1), ('C01567', 1), ('C01568', 1), ('C01569', 1), ('C01570', 1), ('C01571', 1), ('C01572', 1), ('C01573', 0), ('C01574', 1), ('C01575', 1), ('C01576', 1), ('C01577', 1), ('C01578', 1), ('C01579', 1), ('C01580', 1), ('C01581', 1), ('C01582', 1), ('C01583', 1), ('C01584', 1), ('C01585', 1), ('C01586', 1), ('C01587', 1), ('C01588', 1), ('C01589', 1), ('C01590', 1), ('C01591', 1), ('C01592', 1), ('C01593', 1), ('C01594', 1), ('C01595', 1), ('C01596', 1), ('C01597', 1), ('C01598', 1), ('C01599', 1), ('C01600', 1), ('C01601', 1), ('C01602', 1), ('C01603', 1), ('C01604', 1), ('C01605', 1), ('C01606', 1), ('C01607', 1), ('C01608', 1), ('C01609', 1), ('C01610', 1), ('C01611', 1), ('C01612', 1), ('C01613', 1), ('C01614', 1), ('C01615', 1), ('C01616', 1), ('C01617', 1), ('C01618', 1), ('C01619', 1), ('C01620', 1), ('C01621', 1), ('C01622', 1), ('C01623', 1), ('C01624', 1), ('C01625', 1), ('C01626', 1), ('C01627', 1), ('C01628', 1), ('C01629', 1), ('C01630', 1), ('C01631', 1), ('C01632', 1), ('C01633', 1), ('C01634', 1), ('C01635', 1), ('C01636', 1), ('C01637', 1), ('C01638', 0), ('C01639', 1), ('C01640', 1), ('C01641', 1), ('C01642', 1), ('C01643', 0), ('C01644', 1), ('C01645', 0), ('C01646', 1), ('C01647', 1), ('C01648', 1), ('C01649', 1), ('C01650', 1), ('C01651', 1), ('C01652', 1), ('C01653', 1), ('C01654', 1), ('C01655', 1), ('C01656', 1), ('C01657', 1), ('C01658', 1), ('C01659', 0), ('C01660', 1), ('C01661', 1), ('C01662', 1), ('C01663', 1), ('C01664', 1), ('C01665', 1), ('C01666', 1), ('C01667', 1), ('C01668', 1), ('C01669', 1), ('C01670', 1), ('C01671', 1), ('C01672', 1), ('C01673', 1), ('C01674', 1), ('C01675', 1), ('C01676', 1), ('C01677', 1), ('C01678', 1), ('C01679', 1), ('C01680', 1), ('C01681', 1), ('C01682', 0), ('C01683', 1), ('C01684', 1), ('C01685', 1), ('C01686', 1), ('C01687', 0), ('C01688', 1), ('C01689', 1), ('C01690', 1), ('C01691', 1), ('C01692', 1), ('C01693', 1), ('C01694', 1), ('C01695', 1), ('C01696', 1), ('C01697', 1), ('C01698', 1), ('C01699', 1), ('C01700', 1), ('C01701', 1), ('C01702', 1), ('C01703', 1), ('C01704', 1), ('C01705', 1), ('C01706', 1), ('C01707', 1), ('C01708', 0), ('C01709', 1), ('C01710', 1), ('C01711', 0), ('C01712', 1), ('C01713', 1), ('C01714', 1), ('C01715', 1), ('C01716', 1), ('C01717', 1), ('C01718', 1), ('C01719', 0), ('C01720', 1), ('C01721', 1), ('C01722', 1), ('C01723', 1), ('C01724', 1), ('C01725', 1), ('C01726', 1), ('C01727', 1), ('C01728', 1), ('C01729', 1), ('C01730', 1), ('C01731', 1), ('C01732', 1), ('C01733', 1), ('C01734', 0), ('C01735', 1), ('C01736', 1), ('C01737', 1), ('C01738', 1), ('C01739', 1), ('C01740', 1), ('C01741', 1), ('C01742', 1), ('C01743', 1), ('C01744', 1), ('C01745', 1), ('C01746', 1), ('C01747', 1), ('C01748', 1), ('C01749', 1), ('C01750', 1), ('C01751', 1), ('C01752', 0), ('C01753', 1), ('C01754', 1), ('C01755', 1), ('C01756', 1), ('C01757', 1), ('C01758', 1), ('C01759', 1), ('C01760', 1), ('C01761', 1), ('C01762', 1), ('C01763', 1), ('C01764', 1), ('C01765', 1), ('C01766', 1), ('C01767', 1), ('C01768', 1), ('C01769', 1), ('C01770', 1), ('C01771', 1), ('C01772', 1), ('C01773', 1), ('C01774', 1), ('C01775', 1), ('C01776', 1), ('C01777', 1), ('C01778', 1), ('C01779', 1), ('C01780', 1), ('C01781', 1), ('C01782', 1), ('C01783', 1), ('C01784', 0), ('C01785', 1), ('C01786', 1), ('C01787', 0), ('C01788', 1), ('C01789', 1), ('C01790', 1), ('C01791', 1), ('C01792', 1), ('C01793', 1), ('C01794', 1), ('C01795', 1), ('C01796', 0), ('C01797', 1), ('C01798', 1), ('C01799', 1), ('C01800', 1), ('C01801', 1), ('C01802', 1), ('C01803', 1), ('C01804', 1), ('C01805', 0), ('C01806', 1), ('C01807', 1), ('C01808', 0), ('C01809', 1), ('C01810', 1), ('C01811', 1), ('C01812', 1), ('C01813', 1), ('C01814', 1), ('C01815', 1), ('C01816', 1), ('C01817', 1), ('C01818', 0), ('C01819', 0), ('C01820', 1), ('C01821', 1), ('C01822', 1), ('C01823', 1), ('C01824', 1), ('C01826', 1), ('C01827', 1), ('C01828', 1), ('C01829', 1), ('C01830', 1), ('C01831', 1), ('C01832', 1), ('C01833', 1), ('C01834', 0), ('C01835', 1), ('C01836', 1), ('C01837', 1), ('C01838', 1), ('C01839', 1), ('C01840', 1), ('C01841', 1), ('C01842', 1), ('C01843', 0), ('C01844', 1), ('C01845', 0), ('C01846', 1), ('C01847', 1), ('C01848', 1), ('C01849', 1), ('C01850', 1), ('C01851', 1), ('C01852', 1), ('C01853', 1), ('C01854', 0), ('C01855', 1), ('C01856', 1), ('C01857', 1), ('C01858', 1), ('C01859', 1), ('C01860', 1), ('C01861', 1), ('C01862', 1), ('C01863', 1), ('C01864', 1), ('C01865', 1), ('C01866', 1), ('C01867', 1), ('C01868', 1), ('C01869', 1), ('C01870', 0), ('C01871', 1), ('C01872', 1), ('C01873', 1), ('C01874', 1), ('C01875', 1), ('C01876', 1), ('C01877', 1), ('C01878', 0), ('C01879', 1), ('C01880', 1), ('C01881', 1), ('C01882', 1), ('C01883', 1), ('C01884', 1), ('C01885', 1), ('C01886', 1), ('C01887', 1), ('C01888', 1), ('C01889', 1), ('C01890', 1), ('C01891', 1), ('C01892', 1), ('C01893', 1), ('C01894', 1), ('C01895', 1), ('C01896', 1), ('C01897', 1), ('C01898', 1), ('C01899', 1), ('C01900', 1), ('C01901', 1), ('C01902', 1), ('C01903', 1), ('C01904', 1), ('C01905', 1), ('C01906', 1), ('C01907', 1), ('C01908', 1), ('C01909', 1), ('C01910', 1), ('C01911', 1), ('C01912', 1), ('C01913', 1), ('C01914', 1), ('C01915', 1), ('C01916', 1), ('C01917', 1), ('C01918', 1), ('C01919', 1), ('C01920', 1), ('C01921', 1), ('C01922', 1), ('C01923', 1), ('C01924', 1), ('C01925', 1), ('C01926', 1), ('C01927', 1), ('C01928', 1), ('C01929', 1), ('C01930', 1), ('C01931', 1), ('C01932', 1), ('C01933', 1), ('C01934', 1), ('C01935', 1), ('C01936', 1), ('C01937', 1), ('C01938', 0), ('C01939', 1), ('C01940', 1), ('C01941', 1), ('C01942', 1), ('C01943', 1), ('C01944', 1), ('C01945', 1), ('C01946', 1), ('C01947', 1), ('C01948', 1), ('C01949', 1), ('C01950', 1), ('C01951', 1), ('C01952', 0), ('C01953', 1), ('C01954', 1), ('C01955', 1), ('C01956', 1), ('C01957', 1), ('C01958', 1), ('C01959', 1), ('C01960', 1), ('C01961', 1), ('C01962', 1), ('C01963', 1), ('C01964', 0), ('C01965', 0), ('C01966', 1), ('C01967', 0), ('C01968', 1), ('C01969', 1), ('C01970', 1), ('C01971', 1), ('C01972', 1), ('C01973', 1), ('C01974', 1), ('C01975', 1), ('C01976', 1), ('C01977', 0), ('C01978', 0), ('C01979', 1), ('C01980', 1), ('C01981', 1), ('C01982', 1), ('C01983', 1), ('C01984', 1), ('C01985', 1), ('C01986', 0), ('C01987', 1), ('C01988', 1), ('C01989', 1), ('C01990', 1), ('C01991', 1), ('C01992', 1), ('C01993', 1), ('C01994', 1), ('C01995', 1), ('C01996', 1), ('C01997', 1), ('C01998', 1), ('C01999', 0), ('C02000', 1), ('C02001', 0), ('C02002', 1), ('C02003', 1), ('C02004', 1), ('C02005', 1), ('C02006', 1), ('C02007', 1), ('C02008', 1), ('C02009', 1), ('C02010', 1), ('C02011', 1), ('C02012', 1), ('C02013', 1), ('C02014', 1), ('C02015', 1), ('C02016', 1), ('C02017', 1), ('C02018', 1), ('C02019', 0), ('C02020', 1), ('C02021', 1), ('C02022', 0), ('C02023', 1), ('C02024', 1), ('C02025', 1), ('C02026', 1), ('C02027', 1), ('C02028', 1), ('C02029', 1), ('C02030', 1), ('C02031', 1), ('C02032', 1), ('C02033', 1), ('C02034', 1), ('C02035', 1), ('C02036', 1), ('C02037', 1), ('C02038', 1), ('C02039', 1), ('C02040', 1), ('C02041', 1), ('C02042', 0), ('C02043', 1), ('C02044', 1), ('C02045', 1), ('C02046', 1), ('C02047', 1), ('C02048', 1), ('C02049', 1), ('C02050', 1), ('C02051', 1), ('C02052', 1), ('C02053', 1), ('C02054', 0), ('C02055', 1), ('C02056', 1), ('C02057', 1), ('C02058', 1), ('C02059', 1), ('C02060', 1), ('C02061', 1), ('C02062', 1), ('C02063', 1), ('C02064', 1), ('C02065', 1), ('C02066', 1), ('C02067', 1), ('C02068', 1), ('C02069', 1), ('C02070', 1), ('C02071', 1), ('C02072', 1), ('C02073', 1), ('C02074', 1), ('C02075', 1), ('C02076', 1), ('C02077', 1), ('C02078', 1), ('C02079', 1), ('C02080', 1), ('C02081', 1), ('C02082', 1), ('C02083', 1), ('C02084', 1), ('C02085', 1), ('C02086', 1), ('C02087', 1), ('C02088', 1), ('C02089', 1), ('C02090', 1), ('C02091', 1), ('C02092', 1), ('C02093', 1), ('C02094', 1), ('C02095', 1), ('C02096', 1), ('C02097', 1), ('C02098', 1), ('C02099', 1), ('C02100', 1), ('C02101', 1), ('C02102', 1), ('C02103', 1), ('C02104', 1), ('C02105', 1), ('C02106', 0), ('C02107', 1), ('C02108', 1), ('C02109', 1), ('C02110', 1), ('C02111', 0), ('C02112', 1), ('C02113', 1), ('C02114', 1), ('C02115', 1), ('C02116', 1), ('C02117', 1), ('C02118', 1), ('C02119', 1), ('C02120', 1), ('C02121', 0), ('C02122', 1), ('C02123', 1), ('C02124', 1), ('C02125', 1), ('C02126', 1), ('C02127', 1), ('C02128', 1), ('C02129', 1), ('C02130', 1), ('C02131', 1), ('C02132', 1), ('C02133', 1), ('C02134', 1), ('C02135', 1), ('C02136', 1), ('C02137', 1), ('C02138', 1), ('C02139', 1), ('C02140', 1), ('C02141', 1), ('C02142', 1), ('C02143', 1), ('C02144', 1), ('C02145', 1), ('C02146', 0), ('C02147', 0), ('C02148', 1), ('C02149', 0), ('C02150', 1), ('C02151', 1), ('C02152', 1), ('C02153', 0), ('C02154', 1), ('C02155', 1), ('C02156', 1), ('C02157', 1), ('C02158', 1), ('C02159', 1), ('C02160', 1), ('C02161', 1), ('C02162', 1), ('C02163', 1), ('C02164', 1), ('C02165', 1), ('C02166', 1), ('C02167', 1), ('C02168', 1), ('C02169', 1), ('C02170', 0), ('C02171', 1), ('C02172', 1), ('C02173', 1), ('C02174', 1), ('C02175', 1), ('C02176', 1), ('C02177', 1), ('C02178', 1), ('C02179', 1), ('C02180', 1), ('C02181', 1), ('C02182', 1), ('C02183', 0), ('C02184', 1), ('C02185', 1), ('C02186', 1), ('C02187', 1), ('C02188', 1), ('C02189', 1), ('C02190', 1), ('C02191', 0), ('C02192', 1), ('C02193', 1), ('C02194', 1), ('C02195', 1), ('C02196', 1), ('C02197', 1), ('C02198', 0), ('C02199', 1), ('C02200', 1), ('C02201', 1), ('C02202', 1), ('C02203', 1), ('C02204', 1), ('C02205', 1), ('C02206', 1), ('C02207', 1), ('C02208', 1), ('C02209', 0), ('C02210', 1), ('C02211', 1), ('C02212', 1), ('C02213', 1), ('C02214', 1), ('C02215', 1), ('C02216', 1), ('C02217', 1), ('C02218', 1), ('C02219', 1), ('C02220', 1), ('C02221', 1), ('C02222', 1), ('C02223', 1), ('C02224', 1), ('C02225', 1), ('C02226', 0), ('C02227', 1), ('C02228', 1), ('C02229', 1), ('C02230', 1), ('C02231', 1), ('C02232', 1), ('C02233', 1), ('C02234', 1), ('C02235', 0), ('C02236', 1), ('C02237', 1), ('C02238', 1), ('C02239', 1), ('C02240', 1), ('C02241', 1), ('C02242', 1), ('C02243', 1), ('C02244', 1), ('C02245', 1), ('C02246', 1), ('C02247', 1), ('C02248', 1), ('C02249', 1), ('C02250', 0), ('C02251', 1), ('C02252', 1), ('C02253', 1), ('C02254', 1), ('C02255', 1), ('C02256', 1), ('C02257', 1), ('C02258', 1), ('C02259', 1), ('C02260', 1), ('C02261', 1), ('C02262', 1), ('C02263', 1), ('C02264', 1), ('C02265', 1), ('C02266', 1), ('C02267', 0), ('C02268', 1), ('C02269', 0), ('C02270', 1), ('C02271', 1), ('C02272', 1), ('C02273', 1), ('C02274', 1), ('C02275', 1), ('C02276', 1), ('C02277', 1), ('C02278', 1), ('C02279', 1), ('C02280', 1), ('C02281', 1), ('C02282', 1), ('C02283', 1), ('C02284', 1), ('C02285', 1), ('C02286', 1), ('C02287', 1), ('C02288', 1), ('C02289', 1), ('C02290', 1), ('C02291', 1), ('C02292', 1), ('C02293', 1), ('C02294', 1), ('C02295', 1), ('C02296', 1), ('C02297', 1), ('C02298', 1), ('C02299', 0), ('C02300', 1), ('C02301', 1), ('C02302', 1), ('C02303', 1), ('C02304', 1), ('C02305', 1), ('C02306', 1), ('C02307', 1), ('C02308', 1), ('C02309', 1), ('C02310', 1), ('C02311', 1), ('C02312', 1), ('C02313', 1), ('C02314', 1), ('C02315', 1), ('C02316', 1), ('C02317', 1), ('C02318', 1), ('C02319', 1), ('C02320', 1), ('C02321', 1), ('C02322', 0), ('C02323', 1), ('C02324', 0), ('C02325', 1), ('C02326', 1), ('C02327', 1), ('C02328', 1), ('C02329', 1), ('C02330', 1), ('C02331', 1), ('C02332', 1), ('C02333', 1), ('C02334', 1), ('C02335', 1), ('C02336', 1), ('C02337', 1), ('C02338', 1), ('C02339', 1), ('C02340', 1), ('C02341', 1), ('C02342', 0), ('C02343', 0), ('C02344', 1), ('C02345', 1), ('C02346', 1), ('C02347', 1), ('C02348', 1), ('C02349', 1), ('C02350', 1), ('C02351', 1), ('C02352', 1), ('C02353', 1), ('C02354', 1), ('C02355', 1), ('C02356', 1), ('C02357', 1), ('C02358', 1), ('C02359', 1), ('C02360', 1), ('C02361', 1), ('C02362', 1), ('C02363', 1), ('C02364', 1), ('C02365', 1), ('C02366', 1), ('C02367', 1), ('C02368', 1), ('C02369', 1), ('C02370', 1), ('C02371', 1), ('C02372', 1), ('C02373', 1), ('C02374', 0), ('C02375', 1), ('C02376', 0), ('C02377', 1), ('C02378', 1), ('C02379', 1), ('C02380', 1), ('C02381', 1), ('C02382', 1), ('C02383', 1), ('C02384', 1), ('C02385', 1), ('C02386', 1), ('C02387', 1), ('C02388', 1), ('C02389', 1), ('C02390', 1), ('C02391', 1), ('C02392', 1), ('C02393', 1), ('C02394', 1), ('C02395', 1), ('C02396', 1), ('C02397', 1), ('C02398', 1), ('C02399', 1), ('C02400', 1), ('C02401', 1), ('C02402', 1), ('C02403', 1), ('C02404', 1), ('C02405', 1), ('C02406', 1), ('C02407', 0), ('C02408', 1), ('C02409', 0), ('C02410', 1), ('C02411', 1), ('C02412', 1), ('C02413', 1), ('C02414', 1), ('C02415', 1), ('C02416', 1), ('C02417', 1), ('C02418', 1), ('C02419', 0), ('C02420', 0), ('C02421', 1), ('C02422', 1), ('C02423', 1), ('C02424', 1), ('C02425', 1), ('C02426', 1), ('C02427', 1), ('C02428', 1), ('C02429', 1), ('C02430', 1), ('C02431', 1), ('C02432', 1), ('C02433', 1), ('C02434', 1), ('C02435', 0), ('C02436', 1), ('C02437', 1), ('C02438', 1), ('C02439', 1), ('C02440', 1), ('C02441', 1), ('C02442', 1), ('C02443', 1), ('C02444', 1), ('C02445', 1), ('C02446', 1), ('C02447', 1), ('C02448', 1), ('C02449', 1), ('C02450', 1), ('C02451', 0), ('C02452', 1), ('C02453', 0), ('C02454', 0), ('C02455', 1), ('C02456', 1), ('C02457', 1), ('C02458', 1), ('C02459', 1), ('C02460', 1), ('C02461', 1), ('C02462', 1), ('C02463', 1), ('C02464', 1), ('C02465', 1), ('C02466', 1), ('C02467', 1), ('C02468', 1), ('C02469', 1), ('C02470', 1), ('C02471', 1), ('C02472', 1), ('C02473', 1), ('C02474', 1), ('C02475', 1), ('C02476', 1), ('C02477', 1), ('C02478', 1), ('C02479', 1), ('C02480', 1), ('C02481', 0), ('C02482', 1), ('C02483', 0), ('C02484', 1), ('C02485', 1), ('C02486', 1), ('C02487', 1), ('C02488', 1), ('C02489', 1), ('C02490', 1), ('C02491', 1), ('C02492', 1), ('C02493', 1), ('C02494', 1), ('C02495', 1), ('C02496', 1), ('C02497', 1), ('C02498', 1), ('C02499', 1), ('C02500', 1), ('C02501', 1), ('C02502', 0), ('C02503', 1), ('C02504', 1), ('C02505', 1), ('C02506', 1), ('C02507', 1), ('C02508', 1), ('C02509', 1), ('C02510', 1), ('C02511', 1), ('C02512', 1), ('C02513', 1), ('C02514', 1), ('C02515', 1), ('C02516', 1), ('C02517', 1), ('C02518', 1), ('C02519', 1), ('C02520', 1), ('C02521', 1), ('C02522', 1), ('C02523', 1), ('C02524', 0), ('C02525', 1), ('C02526', 0), ('C02527', 1), ('C02528', 1), ('C02529', 1), ('C02530', 1), ('C02531', 1), ('C02532', 1), ('C02533', 1), ('C02534', 1), ('C02535', 1), ('C02536', 1), ('C02537', 1), ('C02538', 1), ('C02539', 1), ('C02540', 1), ('C02541', 1), ('C02542', 1), ('C02543', 1), ('C02544', 1), ('C02545', 1), ('C02546', 1), ('C02547', 1), ('C02548', 1), ('C02549', 1), ('C02550', 1), ('C02551', 1), ('C02552', 1), ('C02553', 1), ('C02554', 1), ('C02555', 1), ('C02556', 1), ('C02557', 1), ('C02558', 1), ('C02559', 1), ('C02560', 1), ('C02561', 0), ('C02562', 1), ('C02563', 1), ('C02564', 1), ('C02565', 1), ('C02566', 1), ('C02567', 1), ('C02568', 1), ('C02569', 1), ('C02570', 1), ('C02571', 1), ('C02572', 1), ('C02573', 0), ('C02574', 1), ('C02575', 1), ('C02576', 0), ('C02577', 1), ('C02578', 1), ('C02579', 1), ('C02580', 1), ('C02581', 1), ('C02582', 1), ('C02583', 1), ('C02584', 1), ('C02585', 1), ('C02586', 1), ('C02587', 1), ('C02588', 1), ('C02589', 1), ('C02590', 1), ('C02591', 0), ('C02592', 0), ('C02593', 1), ('C02594', 1), ('C02595', 1), ('C02596', 1), ('C02597', 1), ('C02598', 1), ('C02599', 1), ('C02600', 1), ('C02601', 1), ('C02602', 1), ('C02603', 1), ('C02604', 1), ('C02605', 1), ('C02606', 1), ('C02607', 1), ('C02608', 1), ('C02609', 1), ('C02610', 1), ('C02611', 1), ('C02612', 1), ('C02613', 1), ('C02614', 1), ('C02615', 1), ('C02616', 1), ('C02617', 1), ('C02618', 1), ('C02619', 1), ('C02620', 1), ('C02621', 1), ('C02622', 1), ('C02623', 1), ('C02624', 1), ('C02625', 1), ('C02626', 1), ('C02627', 1), ('C02628', 1), ('C02629', 0), ('C02630', 0), ('C02631', 1), ('C02632', 1), ('C02633', 1), ('C02634', 1), ('C02635', 1), ('C02636', 1), ('C02637', 1), ('C02638', 1), ('C02639', 1), ('C02640', 1), ('C02641', 1), ('C02642', 1), ('C02643', 1), ('C02644', 1), ('C02645', 0), ('C02646', 1), ('C02647', 1), ('C02648', 1), ('C02649', 1), ('C02650', 1), ('C02651', 1), ('C02652', 1), ('C02653', 0), ('C02654', 1), ('C02655', 1), ('C02656', 1), ('C02657', 1), ('C02658', 1), ('C02659', 1), ('C02660', 1), ('C02661', 1), ('C02662', 1), ('C02663', 1), ('C02664', 1), ('C02665', 1), ('C02666', 1), ('C02667', 1), ('C02668', 1), ('C02669', 1), ('C02670', 1), ('C02671', 1), ('C02672', 1), ('C02673', 1), ('C02674', 1), ('C02675', 1), ('C02676', 1), ('C02677', 1), ('C02678', 1), ('C02679', 1), ('C02680', 1), ('C02681', 1), ('C02682', 1), ('C02683', 1), ('C02684', 1), ('C02685', 1), ('C02686', 0), ('C02687', 1), ('C02688', 1), ('C02689', 1), ('C02690', 1), ('C02691', 1), ('C02692', 1), ('C02693', 1), ('C02694', 1), ('C02695', 1), ('C02696', 1), ('C02697', 1), ('C02698', 1), ('C02699', 1), ('C02700', 1), ('C02701', 1), ('C02702', 1), ('C02703', 1), ('C02704', 1), ('C02705', 1), ('C02706', 1), ('C02707', 1), ('C02708', 1), ('C02709', 1), ('C02710', 1), ('C02711', 1), ('C02712', 1), ('C02713', 1), ('C02714', 1), ('C02715', 1), ('C02716', 1), ('C02717', 1), ('C02718', 1), ('C02719', 1), ('C02720', 1), ('C02721', 1), ('C02722', 1), ('C02723', 1), ('C02724', 1), ('C02725', 1), ('C02726', 1), ('C02727', 1), ('C02728', 0), ('C02729', 1), ('C02730', 1), ('C02731', 1), ('C02732', 1), ('C02733', 1), ('C02734', 1), ('C02735', 1), ('C02736', 1), ('C02737', 1), ('C02738', 1), ('C02739', 0), ('C02740', 1), ('C02741', 1), ('C02742', 0), ('C02743', 1), ('C02744', 1), ('C02745', 1), ('C02746', 1), ('C02747', 1), ('C02748', 1), ('C02749', 1), ('C02750', 1), ('C02751', 1), ('C02752', 0), ('C02753', 1), ('C02754', 1), ('C02755', 1), ('C02756', 1), ('C02757', 1), ('C02758', 1), ('C02759', 1), ('C02760', 1), ('C02761', 1), ('C02762', 1), ('C02763', 1), ('C02764', 1), ('C02765', 1), ('C02766', 1), ('C02767', 1), ('C02768', 1), ('C02769', 1), ('C02770', 1), ('C02771', 1), ('C02772', 1), ('C02773', 1), ('C02774', 1), ('C02775', 1), ('C02776', 1), ('C02777', 1), ('C02778', 1), ('C02779', 1), ('C02780', 1), ('C02781', 1), ('C02782', 1), ('C02783', 1), ('C02784', 1), ('C02785', 0), ('C02786', 1), ('C02787', 0), ('C02788', 1), ('C02789', 1), ('C02790', 1), ('C02791', 1), ('C02792', 1), ('C02793', 1), ('C02794', 1), ('C02795', 0), ('C02796', 1), ('C02797', 1), ('C02798', 1), ('C02799', 1), ('C02800', 1), ('C02801', 0), ('C02802', 1), ('C02803', 1), ('C02804', 1), ('C02805', 0), ('C02806', 1), ('C02807', 1), ('C02808', 1), ('C02809', 1), ('C02810', 1), ('C02811', 1), ('C02812', 1), ('C02813', 1), ('C02814', 0), ('C02815', 1), ('C02816', 1), ('C02817', 1), ('C02818', 1), ('C02819', 1), ('C02820', 1), ('C02821', 1), ('C02822', 1), ('C02823', 1), ('C02824', 1), ('C02825', 1), ('C02826', 1), ('C02827', 1), ('C02828', 1), ('C02829', 1), ('C02830', 1), ('C02831', 1), ('C02832', 1), ('C02833', 0), ('C02835', 1), ('C02836', 1), ('C02837', 1), ('C02838', 1), ('C02839', 1), ('C02840', 1), ('C02841', 1), ('C02842', 1), ('C02843', 1), ('C02844', 1), ('C02845', 1), ('C02846', 0), ('C02847', 1), ('C02848', 1), ('C02849', 1), ('C02850', 1), ('C02851', 1), ('C02852', 1), ('C02853', 1), ('C02854', 1), ('C02855', 1), ('C02856', 1), ('C02857', 1), ('C02858', 1), ('C02859', 1), ('C02860', 1), ('C02861', 1), ('C02862', 1), ('C02863', 1), ('C02864', 1), ('C02865', 1), ('C02866', 1), ('C02867', 1), ('C02868', 1), ('C02869', 1), ('C02870', 1), ('C02871', 1), ('C02872', 1), ('C02873', 1), ('C02874', 1), ('C02875', 1), ('C02876', 1), ('C02877', 1), ('C02878', 1), ('C02879', 1), ('C02880', 1), ('C02881', 0), ('C02882', 1), ('C02883', 1), ('C02884', 1), ('C02885', 1), ('C02886', 0), ('C02887', 1), ('C02888', 1), ('C02889', 1), ('C02890', 1), ('C02891', 1), ('C02892', 0), ('C02893', 1), ('C02894', 1), ('C02895', 1), ('C02896', 1), ('C02897', 1), ('C02898', 1), ('C02899', 1), ('C02900', 1), ('C02901', 1), ('C02902', 1), ('C02903', 0), ('C02904', 1), ('C02905', 1), ('C02906', 1), ('C02907', 1), ('C02908', 0), ('C02909', 1), ('C02910', 0), ('C02911', 1), ('C02912', 1), ('C02913', 1), ('C02914', 1), ('C02915', 1), ('C02916', 1), ('C02917', 1), ('C02918', 1), ('C02919', 1), ('C02920', 1), ('C02921', 1), ('C02922', 1), ('C02923', 1), ('C02924', 1), ('C02925', 1), ('C02926', 1), ('C02927', 1), ('C02928', 1), ('C02929', 1), ('C02930', 1), ('C02931', 1), ('C02932', 1), ('C02933', 1), ('C02934', 0), ('C02935', 1), ('C02936', 1), ('C02937', 1), ('C02938', 1), ('C02939', 1), ('C02940', 1), ('C02941', 1), ('C02942', 1), ('C02943', 1), ('C02944', 1), ('C02945', 1), ('C02946', 1), ('C02947', 1), ('C02948', 1), ('C02949', 1), ('C02950', 1), ('C02951', 1), ('C02952', 1), ('C02953', 1), ('C02954', 1), ('C02955', 1), ('C02956', 1), ('C02957', 1), ('C02958', 1), ('C02959', 1), ('C02960', 1), ('C02961', 0), ('C02962', 1), ('C02963', 1), ('C02964', 1), ('C02965', 1), ('C02966', 1), ('C02967', 1), ('C02968', 1), ('C02969', 1), ('C02970', 1), ('C02971', 1), ('C02972', 1), ('C02973', 1), ('C02974', 1), ('C02975', 1), ('C02976', 1), ('C02977', 1), ('C02978', 1), ('C02979', 1), ('C02980', 1), ('C02981', 1), ('C02982', 1), ('C02983', 1), ('C02984', 1), ('C02985', 1), ('C02986', 1), ('C02987', 1), ('C02988', 1), ('C02989', 1), ('C02990', 1), ('C02991', 0), ('C02992', 1), ('C02993', 1), ('C02994', 1), ('C02995', 1), ('C02996', 1), ('C02997', 1), ('C02998', 1), ('C02999', 1), ('C03000', 0), ('C03001', 1), ('C03002', 1), ('C03003', 1), ('C03004', 1), ('C03005', 1), ('C03006', 0), ('C03007', 1), ('C03008', 1), ('C03009', 1), ('C03010', 1), ('C03011', 1), ('C03012', 1), ('C03013', 1), ('C03014', 1), ('C03015', 1), ('C03016', 1), ('C03017', 1), ('C03018', 1), ('C03019', 1), ('C03020', 1), ('C03021', 1), ('C03022', 1), ('C03023', 1), ('C03024', 1), ('C03025', 1), ('C03026', 1), ('C03027', 1), ('C03028', 1), ('C03029', 1), ('C03030', 1), ('C03031', 1), ('C03032', 1), ('C03033', 1), ('C03034', 1), ('C03035', 1), ('C03036', 1), ('C03037', 1), ('C03038', 1), ('C03039', 1), ('C03040', 1), ('C03041', 1), ('C03042', 1), ('C03043', 1), ('C03044', 1), ('C03045', 1), ('C03046', 1), ('C03047', 1), ('C03048', 1), ('C03049', 1), ('C03050', 1), ('C03051', 1), ('C03052', 1), ('C03053', 0), ('C03054', 0), ('C03055', 1), ('C03056', 1), ('C03057', 1), ('C03058', 1), ('C03059', 1), ('C03060', 0), ('C03061', 1), ('C03062', 1), ('C03063', 1), ('C03064', 0), ('C03065', 1), ('C03066', 1), ('C03067', 1), ('C03068', 1), ('C03069', 1), ('C03070', 1), ('C03071', 1), ('C03072', 1), ('C03073', 0), ('C03074', 1), ('C03075', 1), ('C03076', 1), ('C03077', 1), ('C03078', 1), ('C03079', 1), ('C03080', 1), ('C03081', 0), ('C03082', 0), ('C03083', 1), ('C03084', 1), ('C03085', 1), ('C03086', 1), ('C03087', 1), ('C03088', 1), ('C03089', 1), ('C03090', 1), ('C03091', 1), ('C03092', 0), ('C03093', 1), ('C03094', 1), ('C03095', 1), ('C03096', 1), ('C03097', 1), ('C03098', 1), ('C03099', 1), ('C03100', 1), ('C03101', 1), ('C03102', 1), ('C03103', 0), ('C03104', 1), ('C03105', 1), ('C03106', 1), ('C03107', 1), ('C03108', 1), ('C03109', 1), ('C03110', 1), ('C03111', 1), ('C03112', 1), ('C03113', 1), ('C03114', 0), ('C03115', 1), ('C03116', 1), ('C03117', 1), ('C03118', 0), ('C03119', 1), ('C03120', 1), ('C03121', 1), ('C03122', 1), ('C03123', 1), ('C03124', 1), ('C03125', 1), ('C03126', 1), ('C03127', 1), ('C03128', 1), ('C03129', 1), ('C03130', 1), ('C03131', 1), ('C03132', 1), ('C03133', 0), ('C03134', 1), ('C03135', 1), ('C03136', 1), ('C03137', 1), ('C03138', 1), ('C03139', 0), ('C03140', 1), ('C03141', 1), ('C03142', 1), ('C03143', 1), ('C03144', 1), ('C03145', 1), ('C03146', 1), ('C03147', 0), ('C03148', 1), ('C03149', 1), ('C03150', 0), ('C03151', 1), ('C03152', 1), ('C03153', 1), ('C03154', 1), ('C03155', 1), ('C03156', 1), ('C03157', 1), ('C03158', 1), ('C03159', 1), ('C03160', 1), ('C03161', 1), ('C03162', 1), ('C03163', 1), ('C03164', 1), ('C03165', 1), ('C03166', 1), ('C03167', 1), ('C03168', 1), ('C03169', 1), ('C03170', 1), ('C03171', 1), ('C03172', 1), ('C03173', 1), ('C03174', 1), ('C03175', 1), ('C03176', 1), ('C03177', 1), ('C03178', 1), ('C03179', 1), ('C03180', 1), ('C03181', 1), ('C03182', 1), ('C03183', 1), ('C03184', 1), ('C03185', 1), ('C03186', 1), ('C03187', 1), ('C03188', 1), ('C03189', 1), ('C03190', 1), ('C03191', 1), ('C03192', 1), ('C03193', 1), ('C03194', 1), ('C03195', 1), ('C03196', 1), ('C03197', 1), ('C03198', 1), ('C03199', 1), ('C03200', 1), ('C03201', 1), ('C03202', 1), ('C03203', 1), ('C03204', 1), ('C03205', 1), ('C03206', 1), ('C03207', 1), ('C03208', 1), ('C03209', 1), ('C03210', 1), ('C03211', 1), ('C03212', 1), ('C03213', 1), ('C03214', 1), ('C03215', 1), ('C03216', 1), ('C03217', 1), ('C03218', 1), ('C03219', 1), ('C03220', 1), ('C03221', 1), ('C03222', 1), ('C03223', 1), ('C03224', 1), ('C03225', 1), ('C03226', 1), ('C03227', 1), ('C03228', 1), ('C03229', 1), ('C03230', 0), ('C03231', 1), ('C03232', 1), ('C03233', 1), ('C03234', 1), ('C03235', 1), ('C03236', 1), ('C03237', 1), ('C03238', 1), ('C03239', 1), ('C03240', 1), ('C03241', 1), ('C03242', 0), ('C03243', 1), ('C03244', 1), ('C03245', 1), ('C03246', 1), ('C03247', 1), ('C03248', 1), ('C03249', 1), ('C03250', 1), ('C03251', 1), ('C03252', 1), ('C03253', 1), ('C03254', 1), ('C03255', 1), ('C03256', 1), ('C03257', 1), ('C03258', 1), ('C03259', 0), ('C03260', 1), ('C03261', 1), ('C03262', 1), ('C03263', 1), ('C03264', 1), ('C03265', 1), ('C03266', 1), ('C03267', 1), ('C03268', 1), ('C03269', 1), ('C03270', 1), ('C03271', 0), ('C03272', 1), ('C03273', 1), ('C03274', 1), ('C03275', 1), ('C03276', 1), ('C03277', 1), ('C03278', 1), ('C03279', 1), ('C03280', 1), ('C03281', 1), ('C03282', 1), ('C03283', 1), ('C03284', 1), ('C03285', 1), ('C03286', 1), ('C03287', 1), ('C03288', 0), ('C03289', 0), ('C03290', 1), ('C03291', 1), ('C03292', 1), ('C03293', 1), ('C03294', 1), ('C03295', 1), ('C03296', 1), ('C03297', 1), ('C03298', 1), ('C03299', 1), ('C03300', 1), ('C03301', 0), ('C03302', 1), ('C03303', 1), ('C03304', 1), ('C03305', 1), ('C03306', 1), ('C03307', 1), ('C03308', 1), ('C03309', 1), ('C03310', 1), ('C03311', 1), ('C03312', 1), ('C03313', 1), ('C03314', 1), ('C03315', 1), ('C03316', 1), ('C03317', 1), ('C03318', 1), ('C03319', 1), ('C03320', 1), ('C03321', 1), ('C03322', 1), ('C03323', 1), ('C03324', 1), ('C03325', 1), ('C03326', 1), ('C03327', 1), ('C03328', 1), ('C03329', 1), ('C03330', 1), ('C03331', 1), ('C03332', 1), ('C03333', 1), ('C03334', 0), ('C03335', 1), ('C03336', 1), ('C03338', 1), ('C03339', 0), ('C03340', 1), ('C03341', 1), ('C03342', 1), ('C03343', 0), ('C03344', 0), ('C03345', 1), ('C03346', 1), ('C03347', 1), ('C03348', 1), ('C03349', 1), ('C03350', 0), ('C03351', 1), ('C03352', 1), ('C03353', 0), ('C03354', 1), ('C03355', 1), ('C03356', 1), ('C03357', 0), ('C03358', 1), ('C03359', 1), ('C03360', 1), ('C03361', 1), ('C03362', 0), ('C03363', 1), ('C03364', 1), ('C03365', 0), ('C03366', 1), ('C03367', 1), ('C03368', 1), ('C03369', 1), ('C03370', 1), ('C03371', 1), ('C03372', 1), ('C03373', 1), ('C03374', 1), ('C03375', 1), ('C03376', 1), ('C03377', 1), ('C03378', 1), ('C03379', 1), ('C03380', 1), ('C03381', 1), ('C03382', 1), ('C03383', 1), ('C03384', 1), ('C03385', 1), ('C03386', 1), ('C03387', 1), ('C03388', 1), ('C03389', 1), ('C03390', 0), ('C03391', 1), ('C03392', 1), ('C03393', 1), ('C03394', 1), ('C03395', 1), ('C03396', 0), ('C03397', 1), ('C03398', 1), ('C03399', 1), ('C03400', 0), ('C03401', 1), ('C03402', 1), ('C03403', 1), ('C03404', 1), ('C03405', 1), ('C03406', 1), ('C03407', 1), ('C03408', 1), ('C03409', 1), ('C03410', 0), ('C03411', 1), ('C03412', 1), ('C03413', 1), ('C03414', 0), ('C03415', 1), ('C03416', 1), ('C03417', 1), ('C03418', 1), ('C03419', 1), ('C03420', 0), ('C03421', 1), ('C03422', 1), ('C03423', 1), ('C03424', 1), ('C03425', 1), ('C03426', 1), ('C03427', 1), ('C03428', 1), ('C03429', 1), ('C03430', 1), ('C03431', 1), ('C03432', 1), ('C03433', 1), ('C03434', 0), ('C03435', 1), ('C03436', 0), ('C03437', 1), ('C03438', 1), ('C03439', 1), ('C03440', 1), ('C03441', 1), ('C03442', 1), ('C03443', 1), ('C03444', 1), ('C03445', 1), ('C03446', 1), ('C03447', 1), ('C03448', 1), ('C03449', 0), ('C03450', 1), ('C03451', 1), ('C03452', 1), ('C03453', 1), ('C03454', 0), ('C03455', 1), ('C03456', 1), ('C03457', 1), ('C03458', 1), ('C03459', 1), ('C03460', 1), ('C03461', 1), ('C03462', 1), ('C03463', 1), ('C03464', 1), ('C03465', 0), ('C03466', 1), ('C03467', 1), ('C03468', 1), ('C03469', 1), ('C03470', 1), ('C03471', 1), ('C03472', 1), ('C03473', 1), ('C03474', 1), ('C03475', 1), ('C03476', 1), ('C03477', 1), ('C03478', 0), ('C03479', 0), ('C03480', 1), ('C03481', 1), ('C03482', 1), ('C03483', 1), ('C03484', 1), ('C03485', 1), ('C03486', 1), ('C03487', 1), ('C03488', 1), ('C03489', 1), ('C03490', 1), ('C03491', 1), ('C03492', 1), ('C03493', 1), ('C03494', 1), ('C03495', 1), ('C03496', 1), ('C03497', 1), ('C03498', 1), ('C03499', 1), ('C03500', 1), ('C03501', 1), ('C03502', 0), ('C03503', 0), ('C03504', 1), ('C03505', 1), ('C03506', 1), ('C03507', 1), ('C03508', 0), ('C03509', 1), ('C03510', 1), ('C03511', 1), ('C03512', 1), ('C03513', 1), ('C03514', 1), ('C03515', 1), ('C03516', 1), ('C03517', 1), ('C03518', 1), ('C03519', 1), ('C03520', 0), ('C03521', 0), ('C03522', 1), ('C03523', 1), ('C03524', 1), ('C03525', 1), ('C03526', 1), ('C03527', 1), ('C03528', 1), ('C03529', 1), ('C03530', 1), ('C03531', 0), ('C03532', 0), ('C03533', 1), ('C03534', 1), ('C03535', 1), ('C03536', 1), ('C03537', 0), ('C03538', 1), ('C03539', 1), ('C03540', 0), ('C03541', 1), ('C03542', 1), ('C03543', 1), ('C03544', 1), ('C03545', 1), ('C03546', 1), ('C03547', 0), ('C03548', 1), ('C03549', 1), ('C03550', 1), ('C03551', 1), ('C03552', 1), ('C03553', 1), ('C03554', 0), ('C03555', 1), ('C03556', 1), ('C03557', 0), ('C03558', 1), ('C03559', 1), ('C03560', 1), ('C03561', 1), ('C03562', 1), ('C03563', 1), ('C03564', 1), ('C03565', 1), ('C03566', 1), ('C03567', 1), ('C03568', 1), ('C03569', 1), ('C03570', 0), ('C03571', 1), ('C03572', 1), ('C03573', 1), ('C03574', 1), ('C03575', 1), ('C03576', 1), ('C03577', 1), ('C03578', 1), ('C03579', 1), ('C03580', 1), ('C03581', 1), ('C03582', 1), ('C03583', 1), ('C03584', 1), ('C03585', 1), ('C03586', 1), ('C03587', 1), ('C03588', 1), ('C03589', 1), ('C03590', 1), ('C03591', 1), ('C03592', 1), ('C03593', 1), ('C03594', 0), ('C03595', 1), ('C03596', 1), ('C03597', 1), ('C03598', 1), ('C03599', 1), ('C03600', 1), ('C03601', 1), ('C03602', 1), ('C03603', 1), ('C03604', 1), ('C03605', 1), ('C03606', 1), ('C03607', 1), ('C03608', 1), ('C03609', 1), ('C03610', 1), ('C03611', 1), ('C03612', 0), ('C03613', 1), ('C03614', 1), ('C03615', 1), ('C03616', 1), ('C03617', 1), ('C03618', 1), ('C03619', 1), ('C03620', 0), ('C03621', 1), ('C03622', 1), ('C03623', 1), ('C03624', 1), ('C03625', 1), ('C03626', 1), ('C03627', 1), ('C03628', 1), ('C03629', 0), ('C03630', 1), ('C03631', 1), ('C03632', 1), ('C03633', 1), ('C03634', 1), ('C03635', 1), ('C03636', 1), ('C03637', 1), ('C03638', 1), ('C03639', 1), ('C03640', 1), ('C03641', 1), ('C03642', 1), ('C03643', 1), ('C03644', 1), ('C03645', 1), ('C03646', 1), ('C03647', 1), ('C03648', 0), ('C03649', 1), ('C03650', 1), ('C03651', 1), ('C03652', 1), ('C03653', 1), ('C03654', 1), ('C03655', 1), ('C03656', 1), ('C03657', 1), ('C03658', 1), ('C03659', 1), ('C03660', 0), ('C03661', 1), ('C03662', 1), ('C03663', 1), ('C03664', 1), ('C03665', 1), ('C03666', 1), ('C03667', 1), ('C03668', 1), ('C03669', 1), ('C03670', 1), ('C03671', 1), ('C03672', 1), ('C03673', 1), ('C03674', 1), ('C03675', 1), ('C03676', 1), ('C03677', 1), ('C03678', 1), ('C03679', 1), ('C03680', 1), ('C03681', 1), ('C03682', 1), ('C03683', 1), ('C03684', 1), ('C03685', 1), ('C03686', 1), ('C03687', 1), ('C03688', 1), ('C03689', 1), ('C03690', 1), ('C03691', 1), ('C03692', 1), ('C03693', 1), ('C03694', 1), ('C03695', 1), ('C03696', 1), ('C03697', 1), ('C03698', 0), ('C03699', 1), ('C03700', 0), ('C03701', 1), ('C03702', 1), ('C03703', 1), ('C03704', 1), ('C03706', 1), ('C03707', 1), ('C03708', 1), ('C03709', 1), ('C03710', 1), ('C03711', 1), ('C03712', 1), ('C03713', 1), ('C03714', 1), ('C03715', 1), ('C03716', 1), ('C03717', 1), ('C03718', 1), ('C03719', 1), ('C03720', 0), ('C03721', 1), ('C03722', 1), ('C03723', 1), ('C03724', 1), ('C03725', 1), ('C03726', 1), ('C03727', 1), ('C03728', 1), ('C03729', 1), ('C03730', 1), ('C03731', 1), ('C03732', 1), ('C03733', 1), ('C03734', 1), ('C03735', 1), ('C03736', 1), ('C03737', 1), ('C03738', 1), ('C03739', 1), ('C03740', 1), ('C03741', 1), ('C03742', 1), ('C03743', 1), ('C03744', 1), ('C03745', 1), ('C03746', 1), ('C03747', 1), ('C03748', 1), ('C03749', 1), ('C03750', 1), ('C03751', 1), ('C03752', 1), ('C03753', 1), ('C03754', 1), ('C03755', 0), ('C03756', 1), ('C03757', 1), ('C03758', 1), ('C03759', 1), ('C03760', 1), ('C03761', 0), ('C03762', 1), ('C03763', 1), ('C03764', 1), ('C03765', 1), ('C03766', 1), ('C03767', 1), ('C03768', 1), ('C03769', 1), ('C03770', 1), ('C03771', 1), ('C03772', 1), ('C03773', 0), ('C03774', 1), ('C03776', 1), ('C03777', 1), ('C03778', 1), ('C03779', 1), ('C03780', 1), ('C03781', 1), ('C03782', 1), ('C03783', 1), ('C03784', 1), ('C03785', 1), ('C03786', 0), ('C03787', 1), ('C03788', 1), ('C03789', 1), ('C03790', 0), ('C03791', 1), ('C03792', 1), ('C03793', 1), ('C03794', 1), ('C03795', 1), ('C03796', 1), ('C03797', 1), ('C03798', 1), ('C03799', 1), ('C03800', 1), ('C03801', 1), ('C03802', 1), ('C03803', 1), ('C03804', 1), ('C03805', 1), ('C03806', 1), ('C03807', 1), ('C03808', 1), ('C03809', 1), ('C03810', 1), ('C03811', 1), ('C03812', 1), ('C03813', 1), ('C03814', 1), ('C03815', 1), ('C03816', 1), ('C03817', 1), ('C03818', 1), ('C03819', 1), ('C03820', 1), ('C03821', 1), ('C03822', 1), ('C03823', 0), ('C03824', 1), ('C03825', 1), ('C03826', 1), ('C03827', 0), ('C03828', 1), ('C03829', 1), ('C03830', 1), ('C03831', 1), ('C03832', 1), ('C03833', 1), ('C03834', 1), ('C03835', 1), ('C03836', 1), ('C03837', 1), ('C03839', 1), ('C03840', 1), ('C03841', 1), ('C03842', 1), ('C03843', 1), ('C03844', 1), ('C03845', 1), ('C03846', 1), ('C03847', 0), ('C03848', 1), ('C03849', 1), ('C03850', 1), ('C03851', 1), ('C03852', 1), ('C03853', 1), ('C03854', 1), ('C03855', 1), ('C03856', 1), ('C03857', 0), ('C03858', 0), ('C03859', 1), ('C03860', 1), ('C03862', 1), ('C03863', 1), ('C03864', 0), ('C03865', 1), ('C03866', 1), ('C03867', 1), ('C03868', 1), ('C03869', 1), ('C03870', 0), ('C03871', 1), ('C03872', 1), ('C03873', 1), ('C03874', 1), ('C03875', 0), ('C03876', 1), ('C03877', 1), ('C03878', 1), ('C03879', 1), ('C03880', 1), ('C03881', 1), ('C03882', 1), ('C03883', 1), ('C03884', 1), ('C03885', 1), ('C03886', 1), ('C03887', 1), ('C03888', 1), ('C03889', 1), ('C03890', 1), ('C03891', 1), ('C03892', 1), ('C03893', 1), ('C03894', 1), ('C03895', 1), ('C03896', 1), ('C03897', 0), ('C03898', 1), ('C03899', 1), ('C03900', 1), ('C03901', 1), ('C03902', 1), ('C03903', 0), ('C03904', 1), ('C03905', 1), ('C03906', 1), ('C03907', 1), ('C03908', 1), ('C03909', 1), ('C03910', 1), ('C03911', 1), ('C03912', 1), ('C03913', 0), ('C03914', 0), ('C03915', 1), ('C03916', 1), ('C03917', 1), ('C03918', 1), ('C03919', 1), ('C03920', 1), ('C03921', 1), ('C03922', 1), ('C03923', 1), ('C03924', 1), ('C03925', 1), ('C03926', 1), ('C03927', 1), ('C03928', 1), ('C03929', 1), ('C03930', 1), ('C03931', 1), ('C03932', 1), ('C03933', 1), ('C03934', 1), ('C03935', 1), ('C03936', 1), ('C03937', 1), ('C03938', 1), ('C03939', 0), ('C03940', 1), ('C03941', 1), ('C03942', 1), ('C03943', 1), ('C03944', 1), ('C03945', 1), ('C03946', 0), ('C03947', 1), ('C03948', 0), ('C03949', 1), ('C03950', 1), ('C03951', 0), ('C03952', 1), ('C03953', 1), ('C03954', 1), ('C03955', 1), ('C03956', 1), ('C03957', 1), ('C03958', 1), ('C03959', 1), ('C03960', 1), ('C03961', 1), ('C03962', 0), ('C03963', 1), ('C03964', 1), ('C03965', 1), ('C03966', 1), ('C03967', 1), ('C03968', 1), ('C03969', 1), ('C03970', 1), ('C03971', 1), ('C03972', 0), ('C03973', 1), ('C03974', 1), ('C03975', 0), ('C03976', 1), ('C03977', 1), ('C03978', 1), ('C03979', 1), ('C03980', 0), ('C03981', 0), ('C03982', 1), ('C03983', 1), ('C03984', 1), ('C03985', 1), ('C03986', 1), ('C03987', 1), ('C03988', 1), ('C03989', 1), ('C03990', 1), ('C03991', 1), ('C03992', 1), ('C03993', 1), ('C03994', 1), ('C03995', 1), ('C03996', 1), ('C03997', 1), ('C03998', 1), ('C03999', 1)]\n",
      "[('C00048', 0), ('C00239', 1), ('C00605', 1), ('C00796', 0), ('C00810', 1), ('C00837', 1), ('C01156', 1), ('C01298', 1), ('C01340', 0), ('C01477', 1), ('C01825', 1), ('C02834', 1), ('C03337', 1), ('C03705', 1), ('C03775', 1), ('C03838', 1), ('C03861', 0)]\n"
     ]
    }
   ],
   "source": [
    "#Merge predictions with IDs\n",
    "\n",
    "NaO_preds = list(zip(test_ids_l2_NaO, predictions_ontest_NaO_2nd_layer))\n",
    "print(NaO_preds)\n",
    "\n",
    "AaN_preds = list(zip(test_ids_l2_AaN, predictions_ontest_AaN_2nd_layer))\n",
    "print(AaN_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00001</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00002</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00003</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00004</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Encoding Predicted\n",
       "0  C00000         1         N\n",
       "1  C00001         1         N\n",
       "2  C00002         1         N\n",
       "3  C00003         1         N\n",
       "4  C00004         1         N"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Normal & Other predictions\n",
    "predictions_NaO = pd.DataFrame(NaO_preds, columns=['ID', 'Encoding'])\n",
    "predictions_NaO.head()\n",
    "\n",
    "def decode_NaO(row):\n",
    "     # 'O'==0, 'N'==1\n",
    "    if row['Encoding'] == 0:\n",
    "        return 'O'\n",
    "    else:\n",
    "        return 'N'\n",
    "\n",
    "predictions_NaO['Predicted'] = predictions_NaO.apply(decode_NaO, axis=1)\n",
    "predictions_NaO.head()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00048</td>\n",
       "      <td>0</td>\n",
       "      <td>~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00239</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00605</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00796</td>\n",
       "      <td>0</td>\n",
       "      <td>~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00810</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Encoding Predicted\n",
       "0  C00048         0         ~\n",
       "1  C00239         1         A\n",
       "2  C00605         1         A\n",
       "3  C00796         0         ~\n",
       "4  C00810         1         A"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get AF & Noisy predictions\n",
    "predictions_AaN = pd.DataFrame(AaN_preds, columns=['ID', 'Encoding'])\n",
    "predictions_AaN.head()\n",
    "\n",
    "def decode_AaN(row):\n",
    "    # 'A'==1, '~'==0\n",
    "    if row['Encoding'] == 0:\n",
    "        return '~'\n",
    "    else:\n",
    "        return 'A'\n",
    "    \n",
    "predictions_AaN['Predicted'] = predictions_AaN.apply(decode_AaN, axis=1)\n",
    "predictions_AaN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>C01600</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Encoding Predicted\n",
       "1600  C01600         1         A"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Randonly guess the rows that were removed because of missing values\n",
    "# removed_rows_predictions = pd.DataFrame(removed_rows, columns=['ID'])\n",
    "# guesses1 = [1]*removed_rows_predictions.shape[0]\n",
    "# removed_rows_predictions['Encoding'] = guesses1\n",
    "# guesse2 = ['A']*removed_rows_predictions.shape[0]\n",
    "# removed_rows_predictions['Predicted'] = guesse2\n",
    "\n",
    "\n",
    "# removed_rows_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00001</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00002</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00003</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00004</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Encoding Predicted\n",
       "0  C00000         1         N\n",
       "1  C00001         1         N\n",
       "2  C00002         1         N\n",
       "3  C00003         1         N\n",
       "4  C00004         1         N"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Dataframes into one\n",
    "frames = [predictions_NaO, predictions_AaN]\n",
    "predictions = pd.concat(frames)\n",
    "print(predictions.shape)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  Encoding Predicted\n",
      "0     C00000         1         N\n",
      "1     C00001         1         N\n",
      "2     C00002         1         N\n",
      "3     C00003         1         N\n",
      "4     C00004         1         N\n",
      "...      ...       ...       ...\n",
      "3978  C03995         1         N\n",
      "3979  C03996         1         N\n",
      "3980  C03997         1         N\n",
      "3981  C03998         1         N\n",
      "3982  C03999         1         N\n",
      "\n",
      "[4000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure Sorted by ID\n",
    "predictions.sort_values(by=['ID'], inplace=True)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have dropped all rows with missing values \n",
    "# Therefore have 3999 rows instead of the original 4000\n",
    "# WHAT TO DO WHEN FOR SUBMISSION??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop endoding column\n",
    "results = pd.DataFrame(predictions, columns=['ID', 'Predicted'])\n",
    "# Write to file\n",
    "results.to_csv(\"submission_LSTM_FCN_2_layer_framework.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
